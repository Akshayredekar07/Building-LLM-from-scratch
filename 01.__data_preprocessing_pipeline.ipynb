{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d60c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of characters: 20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"total no of characters: {len(raw_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54b49e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:99])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ac7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d637a1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', '!', 'NLP', ',', 'AI', '&', 'ML', ':', \"they're all amazing\", ';', \"aren't they\", '?', \"Let's split\", '-', 'and see', ':', 'how it works.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Wow! NLP, AI & ML: they're all amazing; aren't they? Let's split - and see: how it works.\"\n",
    "\n",
    "pattern = r'([,.:;?_!&\"\\(\\)\\'|\\-]\\s)'\n",
    "\n",
    "result = re.split(pattern, text)\n",
    "\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a561e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ff1c5",
   "metadata": {},
   "source": [
    "**Step 1 – `re.split(...)`**\n",
    "\n",
    "**Pattern**:\n",
    "`([,.:;?_!\"()\\']|--|\\s)`\n",
    "\n",
    "* `[...]` → match any **one** punctuation character inside the brackets\n",
    "* `--` → match a **double dash**\n",
    "* `\\s` → match **whitespace** (space, tab, newline)\n",
    "* **Parentheses** `( ... )` → capture group, so the matched delimiters are also returned in the list\n",
    "\n",
    "**Applied on**: `\"Hello, world!  This is NLP--fun.\"`\n",
    "\n",
    "The split keeps delimiters as separate list items:\n",
    "\n",
    "```\n",
    "[\n",
    "    'Hello', ',', ' ', 'world', '!', ' ', ' ', 'This', ' ', \n",
    "    'is', ' ', 'NLP', '--', 'fun', '.', ''\n",
    "]\n",
    "```\n",
    "**Step 2 – The for loop (list comprehension)**\n",
    "\n",
    "```python\n",
    "[item.strip() for item in preprocessed if item.strip()]\n",
    "```\n",
    "\n",
    "This is **equivalent** to:\n",
    "\n",
    "```python\n",
    "result = []\n",
    "for item in preprocessed:\n",
    "    if item.strip():              # Remove leading/trailing spaces, check if not empty\n",
    "        result.append(item.strip())  # Add stripped version to result\n",
    "```\n",
    "\n",
    "\n",
    "**Dry Run of Loop**\n",
    "\n",
    "**Initial**:\n",
    "`result = []`\n",
    "\n",
    "| `item`    | `item.strip()` | Non-empty? | Action           |\n",
    "| --------- | -------------- | ---------- | ---------------- |\n",
    "| `'Hello'` | `'Hello'`      | Yes        | Append `'Hello'` |\n",
    "| `','`     | `','`          | Yes        | Append `','`     |\n",
    "| `' '`     | `''`           | No         | Skip             |\n",
    "| `'world'` | `'world'`      | Yes        | Append `'world'` |\n",
    "| `'!'`     | `'!'`          | Yes        | Append `'!'`     |\n",
    "| `' '`     | `''`           | No         | Skip             |\n",
    "| `' '`     | `''`           | No         | Skip             |\n",
    "| `'This'`  | `'This'`       | Yes        | Append `'This'`  |\n",
    "| `' '`     | `''`           | No         | Skip             |\n",
    "| `'is'`    | `'is'`         | Yes        | Append `'is'`    |\n",
    "| `' '`     | `''`           | No         | Skip             |\n",
    "| `'NLP'`   | `'NLP'`        | Yes        | Append `'NLP'`   |\n",
    "| `'--'`    | `'--'`         | Yes        | Append `'--'`    |\n",
    "| `'fun'`   | `'fun'`        | Yes        | Append `'fun'`   |\n",
    "| `'.'`     | `'.'`          | Yes        | Append `'.'`     |\n",
    "| `''`      | `''`           | No         | Skip             |\n",
    "\n",
    "\n",
    "\n",
    "**Final `preprocessed` after loop**:\n",
    "\n",
    "```\n",
    "['Hello', ',', 'world', '!', 'This', 'is', 'NLP', '--', 'fun', '.']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904372ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b066ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfe2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270f4cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65e760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f14148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9), ('?', 10), ('A', 11), ('Ah', 12), ('Among', 13), ('And', 14), ('Are', 15), ('Arrt', 16), ('As', 17), ('At', 18), ('Be', 19), ('Begin', 20), ('Burlington', 21), ('But', 22), ('By', 23), ('Carlo', 24), ('Chicago', 25), ('Claude', 26), ('Come', 27), ('Croft', 28), ('Destroyed', 29), ('Devonshire', 30), ('Don', 31), ('Dubarry', 32), ('Emperors', 33), ('Florence', 34), ('For', 35), ('Gallery', 36), ('Gideon', 37), ('Gisburn', 38), ('Gisburns', 39), ('Grafton', 40), ('Greek', 41), ('Grindle', 42), ('Grindles', 43), ('HAD', 44), ('Had', 45), ('Hang', 46), ('Has', 47), ('He', 48), ('Her', 49), ('Hermia', 50), ('His', 51), ('How', 52), ('I', 53), ('If', 54), ('In', 55), ('It', 56), ('Jack', 57), ('Jove', 58), ('Just', 59), ('Lord', 60), ('Made', 61), ('Miss', 62), ('Money', 63), ('Monte', 64), ('Moon-dancers', 65), ('Mr', 66), ('Mrs', 67), ('My', 68), ('Never', 69), ('No', 70), ('Now', 71), ('Nutley', 72), ('Of', 73), ('Oh', 74), ('On', 75), ('Once', 76), ('Only', 77), ('Or', 78), ('Perhaps', 79), ('Poor', 80), ('Professional', 81), ('Renaissance', 82), ('Rickham', 83), ('Riviera', 84), ('Rome', 85), ('Russian', 86), ('Sevres', 87), ('She', 88), ('Stroud', 89), ('Strouds', 90), ('Suddenly', 91), ('That', 92), ('The', 93), ('Then', 94), ('There', 95), ('They', 96), ('This', 97), ('Those', 98), ('Though', 99), ('Thwing', 100), ('Thwings', 101), ('To', 102), ('Usually', 103), ('Venetian', 104), ('Victor', 105), ('Was', 106), ('We', 107), ('Well', 108), ('What', 109), ('When', 110), ('Why', 111), ('Yes', 112), ('You', 113), ('_', 114), ('a', 115), ('abdication', 116), ('able', 117), ('about', 118), ('above', 119), ('abruptly', 120), ('absolute', 121), ('absorbed', 122), ('absurdity', 123), ('academic', 124), ('accuse', 125), ('accustomed', 126), ('across', 127), ('activity', 128), ('add', 129), ('added', 130), ('admirers', 131), ('adopted', 132), ('adulation', 133), ('advance', 134), ('aesthetic', 135), ('affect', 136), ('afraid', 137), ('after', 138), ('afterward', 139), ('again', 140), ('ago', 141), ('ah', 142), ('air', 143), ('alive', 144), ('all', 145), ('almost', 146), ('alone', 147), ('along', 148), ('always', 149), ('am', 150), ('amazement', 151), ('amid', 152), ('among', 153), ('amplest', 154), ('amusing', 155), ('an', 156), ('and', 157), ('another', 158), ('answer', 159), ('answered', 160), ('any', 161), ('anything', 162), ('anywhere', 163), ('apparent', 164), ('apparently', 165), ('appearance', 166), ('appeared', 167), ('appointed', 168), ('are', 169), ('arm', 170), ('arm-chair', 171), ('arm-chairs', 172), ('arms', 173), ('art', 174), ('articles', 175), ('artist', 176), ('as', 177), ('aside', 178), ('asked', 179), ('at', 180), ('atmosphere', 181), ('atom', 182), ('attack', 183), ('attention', 184), ('attitude', 185), ('audacities', 186), ('away', 187), ('awful', 188), ('axioms', 189), ('azaleas', 190), ('back', 191), ('background', 192), ('balance', 193), ('balancing', 194), ('balustraded', 195), ('basking', 196), ('bath-rooms', 197), ('be', 198), ('beaming', 199), ('bean-stalk', 200), ('bear', 201), ('beard', 202), ('beauty', 203), ('became', 204), ('because', 205), ('becoming', 206), ('bed', 207), ('been', 208), ('before', 209), ('began', 210), ('begun', 211), ('behind', 212), ('being', 213), ('believed', 214), ('beneath', 215), ('bespoke', 216), ('better', 217), ('between', 218), ('big', 219), ('bits', 220), ('bitterness', 221), ('blocked', 222), ('born', 223), ('borne', 224), ('boudoir', 225), ('bravura', 226), ('break', 227), ('breaking', 228), ('breathing', 229), ('bric-a-brac', 230), ('briefly', 231), ('brings', 232), ('bronzes', 233), ('brought', 234), ('brown', 235), ('brush', 236), ('bull', 237), ('business', 238), ('but', 239), ('buying', 240), ('by', 241), ('called', 242), ('came', 243), ('can', 244), ('canvas', 245), ('canvases', 246), ('cards', 247), ('care', 248), ('career', 249), ('caught', 250), ('central', 251), ('chair', 252), ('chap', 253), ('characteristic', 254), ('charming', 255), ('cheap', 256), ('check', 257), ('cheeks', 258), ('chest', 259), ('chimney-piece', 260), ('chucked', 261), ('cigar', 262), ('cigarette', 263), ('cigars', 264), ('circulation', 265), ('circumstance', 266), ('circus-clown', 267), ('claimed', 268), ('clasping', 269), ('clear', 270), ('cleverer', 271), ('close', 272), ('clue', 273), ('coat', 274), ('collapsed', 275), ('colour', 276), ('come', 277), ('comfortable', 278), ('coming', 279), ('companion', 280), ('compared', 281), ('complex', 282), ('confident', 283), ('congesting', 284), ('conjugal', 285), ('constraint', 286), ('consummate', 287), ('contended', 288), ('continued', 289), ('corner', 290), ('corrected', 291), ('could', 292), ('couldn', 293), ('count', 294), ('countenance', 295), ('couple', 296), ('course', 297), ('covered', 298), ('craft', 299), ('cried', 300), ('crossed', 301), ('crowned', 302), ('crumbled', 303), ('cry', 304), ('cured', 305), ('curiosity', 306), ('curious', 307), ('current', 308), ('curtains', 309), ('d', 310), ('dabble', 311), ('damask', 312), ('dark', 313), ('dashed', 314), ('day', 315), ('days', 316), ('dead', 317), ('deadening', 318), ('dear', 319), ('deep', 320), ('deerhound', 321), ('degree', 322), ('delicate', 323), ('demand', 324), ('denied', 325), ('deploring', 326), ('deprecating', 327), ('deprecatingly', 328), ('desire', 329), ('destroyed', 330), ('destruction', 331), ('desultory', 332), ('detail', 333), ('diagnosis', 334), ('did', 335), ('didn', 336), ('died', 337), ('dim', 338), ('dimmest', 339), ('dingy', 340), ('dining-room', 341), ('disarming', 342), ('discovery', 343), ('discrimination', 344), ('discussion', 345), ('disdain', 346), ('disdained', 347), ('disease', 348), ('disguised', 349), ('display', 350), ('dissatisfied', 351), ('distinguished', 352), ('distract', 353), ('divert', 354), ('do', 355), ('doesn', 356), ('doing', 357), ('domestic', 358), ('don', 359), ('done', 360), ('donkey', 361), ('down', 362), ('dozen', 363), ('dragged', 364), ('drawing-room', 365), ('drawing-rooms', 366), ('drawn', 367), ('dress-closets', 368), ('drew', 369), ('dropped', 370), ('each', 371), ('earth', 372), ('ease', 373), ('easel', 374), ('easy', 375), ('echoed', 376), ('economy', 377), ('effect', 378), ('effects', 379), ('efforts', 380), ('egregious', 381), ('eighteenth-century', 382), ('elbow', 383), ('elegant', 384), ('else', 385), ('embarrassed', 386), ('enabled', 387), ('end', 388), ('endless', 389), ('enjoy', 390), ('enlightenment', 391), ('enough', 392), ('ensuing', 393), ('equally', 394), ('equanimity', 395), ('escape', 396), ('established', 397), ('etching', 398), ('even', 399), ('event', 400), ('ever', 401), ('everlasting', 402), ('every', 403), ('exasperated', 404), ('except', 405), ('excuse', 406), ('excusing', 407), ('existed', 408), ('expected', 409), ('exquisite', 410), ('exquisitely', 411), ('extenuation', 412), ('exterminating', 413), ('extracting', 414), ('eye', 415), ('eyebrows', 416), ('eyes', 417), ('face', 418), ('faces', 419), ('fact', 420), ('faded', 421), ('failed', 422), ('failure', 423), ('fair', 424), ('faith', 425), ('false', 426), ('familiar', 427), ('famille-verte', 428), ('fancy', 429), ('fashionable', 430), ('fate', 431), ('feather', 432), ('feet', 433), ('fell', 434), ('fellow', 435), ('felt', 436), ('few', 437), ('fewer', 438), ('finality', 439), ('find', 440), ('fingers', 441), ('first', 442), ('fit', 443), ('fitting', 444), ('five', 445), ('flash', 446), ('flashed', 447), ('florid', 448), ('flowers', 449), ('fluently', 450), ('flung', 451), ('follow', 452), ('followed', 453), ('fond', 454), ('footstep', 455), ('for', 456), ('forced', 457), ('forcing', 458), ('forehead', 459), ('foreign', 460), ('foreseen', 461), ('forgive', 462), ('forgotten', 463), ('form', 464), ('formed', 465), ('forming', 466), ('forward', 467), ('fostered', 468), ('found', 469), ('foundations', 470), ('fragment', 471), ('fragments', 472), ('frame', 473), ('frames', 474), ('frequently', 475), ('friend', 476), ('from', 477), ('full', 478), ('fullest', 479), ('furiously', 480), ('furrowed', 481), ('garlanded', 482), ('garlands', 483), ('gave', 484), ('genial', 485), ('genius', 486), ('gesture', 487), ('get', 488), ('getting', 489), ('give', 490), ('given', 491), ('glad', 492), ('glanced', 493), ('glimpse', 494), ('gloried', 495), ('glory', 496), ('go', 497), ('going', 498), ('gone', 499), ('good', 500), ('good-breeding', 501), ('good-humoured', 502), ('got', 503), ('grace', 504), ('gradually', 505), ('gray', 506), ('grayish', 507), ('great', 508), ('greatest', 509), ('greatness', 510), ('grew', 511), ('groping', 512), ('growing', 513), ('had', 514), ('hadn', 515), ('hair', 516), ('half', 517), ('half-light', 518), ('half-mechanically', 519), ('hall', 520), ('hand', 521), ('hands', 522), ('handsome', 523), ('hanging', 524), ('happen', 525), ('happened', 526), ('hard', 527), ('hardly', 528), ('has', 529), ('have', 530), ('haven', 531), ('having', 532), ('he', 533), ('head', 534), ('hear', 535), ('heard', 536), ('heart', 537), ('height', 538), ('her', 539), ('here', 540), ('hermit', 541), ('herself', 542), ('hesitations', 543), ('hide', 544), ('high', 545), ('him', 546), ('himself', 547), ('hint', 548), ('his', 549), ('history', 550), ('holding', 551), ('home', 552), ('honour', 553), ('hooded', 554), ('hostess', 555), ('hot-house', 556), ('hour', 557), ('hours', 558), ('house', 559), ('how', 560), ('hung', 561), ('husband', 562), ('idea', 563), ('idle', 564), ('idling', 565), ('if', 566), ('immediately', 567), ('in', 568), ('incense', 569), ('indifferent', 570), ('inevitable', 571), ('inevitably', 572), ('inflexible', 573), ('insensible', 574), ('insignificant', 575), ('instinctively', 576), ('instructive', 577), ('interesting', 578), ('into', 579), ('ironic', 580), ('irony', 581), ('irrelevance', 582), ('irrevocable', 583), ('is', 584), ('it', 585), ('its', 586), ('itself', 587), ('jardiniere', 588), ('jealousy', 589), ('just', 590), ('keep', 591), ('kept', 592), ('kind', 593), ('knees', 594), ('knew', 595), ('know', 596), ('known', 597), ('laid', 598), ('lair', 599), ('landing', 600), ('language', 601), ('last', 602), ('late', 603), ('later', 604), ('latter', 605), ('laugh', 606), ('laughed', 607), ('lay', 608), ('leading', 609), ('lean', 610), ('learned', 611), ('least', 612), ('leathery', 613), ('leave', 614), ('led', 615), ('left', 616), ('leisure', 617), ('lends', 618), ('lent', 619), ('let', 620), ('lies', 621), ('life', 622), ('life-likeness', 623), ('lift', 624), ('lifted', 625), ('light', 626), ('lightly', 627), ('like', 628), ('liked', 629), ('line', 630), ('lines', 631), ('lingered', 632), ('lips', 633), ('lit', 634), ('little', 635), ('live', 636), ('ll', 637), ('loathing', 638), ('long', 639), ('longed', 640), ('longer', 641), ('look', 642), ('looked', 643), ('looking', 644), ('lose', 645), ('loss', 646), ('lounging', 647), ('lovely', 648), ('lucky', 649), ('lump', 650), ('luncheon-table', 651), ('luxury', 652), ('lying', 653), ('made', 654), ('make', 655), ('man', 656), ('manage', 657), ('managed', 658), ('mantel-piece', 659), ('marble', 660), ('married', 661), ('may', 662), ('me', 663), ('meant', 664), ('mediocrity', 665), ('medium', 666), ('mentioned', 667), ('mere', 668), ('merely', 669), ('met', 670), ('might', 671), ('mighty', 672), ('millionaire', 673), ('mine', 674), ('minute', 675), ('minutes', 676), ('mirrors', 677), ('modest', 678), ('modesty', 679), ('moment', 680), ('money', 681), ('monumental', 682), ('mood', 683), ('morbidly', 684), ('more', 685), ('most', 686), ('mourn', 687), ('mourned', 688), ('moustache', 689), ('moved', 690), ('much', 691), ('muddling', 692), ('multiplied', 693), ('murmur', 694), ('muscles', 695), ('must', 696), ('my', 697), ('myself', 698), ('mysterious', 699), ('naive', 700), ('near', 701), ('nearly', 702), ('negatived', 703), ('nervous', 704), ('nervousness', 705), ('neutral', 706), ('never', 707), ('next', 708), ('no', 709), ('none', 710), ('not', 711), ('note', 712), ('nothing', 713), ('now', 714), ('nymphs', 715), ('oak', 716), ('obituary', 717), ('object', 718), ('objects', 719), ('occurred', 720), ('oddly', 721), ('of', 722), ('off', 723), ('often', 724), ('oh', 725), ('old', 726), ('on', 727), ('once', 728), ('one', 729), ('ones', 730), ('only', 731), ('onto', 732), ('open', 733), ('or', 734), ('other', 735), ('our', 736), ('ourselves', 737), ('out', 738), ('outline', 739), ('oval', 740), ('over', 741), ('own', 742), ('packed', 743), ('paid', 744), ('paint', 745), ('painted', 746), ('painter', 747), ('painting', 748), ('pale', 749), ('paled', 750), ('palm-trees', 751), ('panel', 752), ('panelling', 753), ('pardonable', 754), ('pardoned', 755), ('part', 756), ('passages', 757), ('passing', 758), ('past', 759), ('pastels', 760), ('pathos', 761), ('patient', 762), ('people', 763), ('perceptible', 764), ('perfect', 765), ('persistence', 766), ('persuasively', 767), ('phrase', 768), ('picture', 769), ('pictures', 770), ('pines', 771), ('pink', 772), ('place', 773), ('placed', 774), ('plain', 775), ('platitudes', 776), ('pleased', 777), ('pockets', 778), ('point', 779), ('poised', 780), ('poor', 781), ('portrait', 782), ('posing', 783), ('possessed', 784), ('poverty', 785), ('predicted', 786), ('preliminary', 787), ('presenting', 788), ('prestidigitation', 789), ('pretty', 790), ('previous', 791), ('price', 792), ('pride', 793), ('princely', 794), ('prism', 795), ('problem', 796), ('proclaiming', 797), ('prodigious', 798), ('profusion', 799), ('protest', 800), ('prove', 801), ('public', 802), ('purblind', 803), ('purely', 804), ('pushed', 805), ('put', 806), ('qualities', 807), ('quality', 808), ('queerly', 809), ('question', 810), ('quickly', 811), ('quietly', 812), ('quite', 813), ('quote', 814), ('rain', 815), ('raised', 816), ('random', 817), ('rather', 818), ('re', 819), ('real', 820), ('really', 821), ('reared', 822), ('reason', 823), ('reassurance', 824), ('recovering', 825), ('recreated', 826), ('reflected', 827), ('reflection', 828), ('regrets', 829), ('relatively', 830), ('remained', 831), ('remember', 832), ('reminded', 833), ('repeating', 834), ('represented', 835), ('reproduction', 836), ('resented', 837), ('resolve', 838), ('resources', 839), ('rest', 840), ('rich', 841), ('ridiculous', 842), ('robbed', 843), ('romantic', 844), ('room', 845), ('rose', 846), ('rs', 847), ('rule', 848), ('run', 849), ('s', 850), ('said', 851), ('same', 852), ('satisfaction', 853), ('savour', 854), ('saw', 855), ('say', 856), ('saying', 857), ('says', 858), ('scorn', 859), ('scornful', 860), ('secret', 861), ('see', 862), ('seemed', 863), ('seen', 864), ('self-confident', 865), ('send', 866), ('sensation', 867), ('sensitive', 868), ('sent', 869), ('serious', 870), ('set', 871), ('sex', 872), ('shade', 873), ('shaking', 874), ('shall', 875), ('she', 876), ('shirked', 877), ('short', 878), ('should', 879), ('shoulder', 880), ('shoulders', 881), ('show', 882), ('showed', 883), ('showy', 884), ('shrug', 885), ('shrugged', 886), ('sight', 887), ('sign', 888), ('silent', 889), ('silver', 890), ('similar', 891), ('simpleton', 892), ('simplifications', 893), ('simply', 894), ('since', 895), ('single', 896), ('sitter', 897), ('sitters', 898), ('sketch', 899), ('skill', 900), ('slight', 901), ('slightly', 902), ('slowly', 903), ('small', 904), ('smile', 905), ('smiling', 906), ('sneer', 907), ('so', 908), ('solace', 909), ('some', 910), ('somebody', 911), ('something', 912), ('spacious', 913), ('spaniel', 914), ('speaking-tubes', 915), ('speculations', 916), ('spite', 917), ('splash', 918), ('square', 919), ('stairs', 920), ('stammer', 921), ('stand', 922), ('standing', 923), ('started', 924), ('stay', 925), ('still', 926), ('stocked', 927), ('stood', 928), ('stopped', 929), ('stopping', 930), ('straddling', 931), ('straight', 932), ('strain', 933), ('straining', 934), ('strange', 935), ('straw', 936), ('stream', 937), ('stroke', 938), ('strokes', 939), ('strolled', 940), ('strongest', 941), ('strongly', 942), ('struck', 943), ('studio', 944), ('stuff', 945), ('subject', 946), ('substantial', 947), ('suburban', 948), ('such', 949), ('suddenly', 950), ('suffered', 951), ('sugar', 952), ('suggested', 953), ('sunburn', 954), ('sunburnt', 955), ('sunlit', 956), ('superb', 957), ('sure', 958), ('surest', 959), ('surface', 960), ('surprise', 961), ('surprised', 962), ('surrounded', 963), ('suspected', 964), ('sweetly', 965), ('sweetness', 966), ('swelling', 967), ('swept', 968), ('swum', 969), ('t', 970), ('table', 971), ('take', 972), ('taken', 973), ('talking', 974), ('tea', 975), ('tears', 976), ('technicalities', 977), ('technique', 978), ('tell', 979), ('tells', 980), ('tempting', 981), ('terra-cotta', 982), ('terrace', 983), ('terraces', 984), ('terribly', 985), ('than', 986), ('that', 987), ('the', 988), ('their', 989), ('them', 990), ('then', 991), ('there', 992), ('therefore', 993), ('they', 994), ('thin', 995), ('thing', 996), ('things', 997), ('think', 998), ('this', 999), ('thither', 1000), ('those', 1001), ('though', 1002), ('thought', 1003), ('three', 1004), ('threshold', 1005), ('threw', 1006), ('through', 1007), ('throwing', 1008), ('tie', 1009), ('till', 1010), ('time', 1011), ('timorously', 1012), ('tinge', 1013), ('tips', 1014), ('tired', 1015), ('to', 1016), ('told', 1017), ('tone', 1018), ('tones', 1019), ('too', 1020), ('took', 1021), ('tottering', 1022), ('touched', 1023), ('toward', 1024), ('trace', 1025), ('trade', 1026), ('transmute', 1027), ('traps', 1028), ('travelled', 1029), ('tribute', 1030), ('tributes', 1031), ('tricks', 1032), ('tried', 1033), ('trouser-presses', 1034), ('true', 1035), ('truth', 1036), ('turned', 1037), ('twenty', 1038), ('twenty-four', 1039), ('twice', 1040), ('twirling', 1041), ('unaccountable', 1042), ('uncertain', 1043), ('under', 1044), ('underlay', 1045), ('underneath', 1046), ('understand', 1047), ('unexpected', 1048), ('untouched', 1049), ('unusual', 1050), ('up', 1051), ('up-stream', 1052), ('upon', 1053), ('upset', 1054), ('upstairs', 1055), ('us', 1056), ('used', 1057), ('usual', 1058), ('value', 1059), ('varnishing', 1060), ('vases', 1061), ('ve', 1062), ('veins', 1063), ('velveteen', 1064), ('very', 1065), ('villa', 1066), ('vindicated', 1067), ('virtuosity', 1068), ('vista', 1069), ('vocation', 1070), ('voice', 1071), ('wall', 1072), ('wander', 1073), ('want', 1074), ('wanted', 1075), ('wants', 1076), ('was', 1077), ('wasn', 1078), ('watched', 1079), ('watching', 1080), ('water-colour', 1081), ('waves', 1082), ('way', 1083), ('weekly', 1084), ('weeks', 1085), ('welcome', 1086), ('went', 1087), ('were', 1088), ('what', 1089), ('when', 1090), ('whenever', 1091), ('where', 1092), ('which', 1093), ('while', 1094), ('white', 1095), ('white-panelled', 1096), ('who', 1097), ('whole', 1098), ('whom', 1099), ('why', 1100), ('wide', 1101), ('widow', 1102), ('wife', 1103), ('wild', 1104), ('wincing', 1105), ('window-curtains', 1106), ('wish', 1107), ('with', 1108), ('without', 1109), ('wits', 1110), ('woman', 1111), ('women', 1112), ('won', 1113), ('wonder', 1114), ('wondered', 1115), ('word', 1116), ('work', 1117), ('working', 1118), ('worth', 1119), ('would', 1120), ('wouldn', 1121), ('year', 1122), ('years', 1123), ('yellow', 1124), ('yet', 1125), ('you', 1126), ('younger', 1127), ('your', 1128), ('yourself', 1129)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dc6bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There', 'They', 'This', 'Those', 'Though', 'Thwing', 'Thwings', 'To', 'Usually', 'Venetian', 'Victor', 'Was', 'We', 'Well', 'What', 'When', 'Why', 'Yes', 'You', '_', 'a', 'abdication', 'able', 'about', 'above', 'abruptly', 'absolute', 'absorbed', 'absurdity', 'academic', 'accuse', 'accustomed', 'across', 'activity', 'add', 'added', 'admirers', 'adopted', 'adulation', 'advance', 'aesthetic', 'affect', 'afraid', 'after', 'afterward', 'again', 'ago', 'ah', 'air', 'alive', 'all', 'almost', 'alone', 'along', 'always', 'am', 'amazement', 'amid', 'among', 'amplest', 'amusing', 'an', 'and', 'another', 'answer', 'answered', 'any', 'anything', 'anywhere', 'apparent', 'apparently', 'appearance', 'appeared', 'appointed', 'are', 'arm', 'arm-chair', 'arm-chairs', 'arms', 'art', 'articles', 'artist', 'as', 'aside', 'asked', 'at', 'atmosphere', 'atom', 'attack', 'attention', 'attitude', 'audacities', 'away', 'awful', 'axioms', 'azaleas', 'back', 'background', 'balance', 'balancing', 'balustraded', 'basking', 'bath-rooms', 'be', 'beaming', 'bean-stalk', 'bear', 'beard', 'beauty', 'became', 'because', 'becoming', 'bed', 'been', 'before', 'began', 'begun', 'behind', 'being', 'believed', 'beneath', 'bespoke', 'better', 'between', 'big', 'bits', 'bitterness', 'blocked', 'born', 'borne', 'boudoir', 'bravura', 'break', 'breaking', 'breathing', 'bric-a-brac', 'briefly', 'brings', 'bronzes', 'brought', 'brown', 'brush', 'bull', 'business', 'but', 'buying', 'by', 'called', 'came', 'can', 'canvas', 'canvases', 'cards', 'care', 'career', 'caught', 'central', 'chair', 'chap', 'characteristic', 'charming', 'cheap', 'check', 'cheeks', 'chest', 'chimney-piece', 'chucked', 'cigar', 'cigarette', 'cigars', 'circulation', 'circumstance', 'circus-clown', 'claimed', 'clasping', 'clear', 'cleverer', 'close', 'clue', 'coat', 'collapsed', 'colour', 'come', 'comfortable', 'coming', 'companion', 'compared', 'complex', 'confident', 'congesting', 'conjugal', 'constraint', 'consummate', 'contended', 'continued', 'corner', 'corrected', 'could', 'couldn', 'count', 'countenance', 'couple', 'course', 'covered', 'craft', 'cried', 'crossed', 'crowned', 'crumbled', 'cry', 'cured', 'curiosity', 'curious', 'current', 'curtains', 'd', 'dabble', 'damask', 'dark', 'dashed', 'day', 'days', 'dead', 'deadening', 'dear', 'deep', 'deerhound', 'degree', 'delicate', 'demand', 'denied', 'deploring', 'deprecating', 'deprecatingly', 'desire', 'destroyed', 'destruction', 'desultory', 'detail', 'diagnosis', 'did', 'didn', 'died', 'dim', 'dimmest', 'dingy', 'dining-room', 'disarming', 'discovery', 'discrimination', 'discussion', 'disdain', 'disdained', 'disease', 'disguised', 'display', 'dissatisfied', 'distinguished', 'distract', 'divert', 'do', 'doesn', 'doing', 'domestic', 'don', 'done', 'donkey', 'down', 'dozen', 'dragged', 'drawing-room', 'drawing-rooms', 'drawn', 'dress-closets', 'drew', 'dropped', 'each', 'earth', 'ease', 'easel', 'easy', 'echoed', 'economy', 'effect', 'effects', 'efforts', 'egregious', 'eighteenth-century', 'elbow', 'elegant', 'else', 'embarrassed', 'enabled', 'end', 'endless', 'enjoy', 'enlightenment', 'enough', 'ensuing', 'equally', 'equanimity', 'escape', 'established', 'etching', 'even', 'event', 'ever', 'everlasting', 'every', 'exasperated', 'except', 'excuse', 'excusing', 'existed', 'expected', 'exquisite', 'exquisitely', 'extenuation', 'exterminating', 'extracting', 'eye', 'eyebrows', 'eyes', 'face', 'faces', 'fact', 'faded', 'failed', 'failure', 'fair', 'faith', 'false', 'familiar', 'famille-verte', 'fancy', 'fashionable', 'fate', 'feather', 'feet', 'fell', 'fellow', 'felt', 'few', 'fewer', 'finality', 'find', 'fingers', 'first', 'fit', 'fitting', 'five', 'flash', 'flashed', 'florid', 'flowers', 'fluently', 'flung', 'follow', 'followed', 'fond', 'footstep', 'for', 'forced', 'forcing', 'forehead', 'foreign', 'foreseen', 'forgive', 'forgotten', 'form', 'formed', 'forming', 'forward', 'fostered', 'found', 'foundations', 'fragment', 'fragments', 'frame', 'frames', 'frequently', 'friend', 'from', 'full', 'fullest', 'furiously', 'furrowed', 'garlanded', 'garlands', 'gave', 'genial', 'genius', 'gesture', 'get', 'getting', 'give', 'given', 'glad', 'glanced', 'glimpse', 'gloried', 'glory', 'go', 'going', 'gone', 'good', 'good-breeding', 'good-humoured', 'got', 'grace', 'gradually', 'gray', 'grayish', 'great', 'greatest', 'greatness', 'grew', 'groping', 'growing', 'had', 'hadn', 'hair', 'half', 'half-light', 'half-mechanically', 'hall', 'hand', 'hands', 'handsome', 'hanging', 'happen', 'happened', 'hard', 'hardly', 'has', 'have', 'haven', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'height', 'her', 'here', 'hermit', 'herself', 'hesitations', 'hide', 'high', 'him', 'himself', 'hint', 'his', 'history', 'holding', 'home', 'honour', 'hooded', 'hostess', 'hot-house', 'hour', 'hours', 'house', 'how', 'hung', 'husband', 'idea', 'idle', 'idling', 'if', 'immediately', 'in', 'incense', 'indifferent', 'inevitable', 'inevitably', 'inflexible', 'insensible', 'insignificant', 'instinctively', 'instructive', 'interesting', 'into', 'ironic', 'irony', 'irrelevance', 'irrevocable', 'is', 'it', 'its', 'itself', 'jardiniere', 'jealousy', 'just', 'keep', 'kept', 'kind', 'knees', 'knew', 'know', 'known', 'laid', 'lair', 'landing', 'language', 'last', 'late', 'later', 'latter', 'laugh', 'laughed', 'lay', 'leading', 'lean', 'learned', 'least', 'leathery', 'leave', 'led', 'left', 'leisure', 'lends', 'lent', 'let', 'lies', 'life', 'life-likeness', 'lift', 'lifted', 'light', 'lightly', 'like', 'liked', 'line', 'lines', 'lingered', 'lips', 'lit', 'little', 'live', 'll', 'loathing', 'long', 'longed', 'longer', 'look', 'looked', 'looking', 'lose', 'loss', 'lounging', 'lovely', 'lucky', 'lump', 'luncheon-table', 'luxury', 'lying', 'made', 'make', 'man', 'manage', 'managed', 'mantel-piece', 'marble', 'married', 'may', 'me', 'meant', 'mediocrity', 'medium', 'mentioned', 'mere', 'merely', 'met', 'might', 'mighty', 'millionaire', 'mine', 'minute', 'minutes', 'mirrors', 'modest', 'modesty', 'moment', 'money', 'monumental', 'mood', 'morbidly', 'more', 'most', 'mourn', 'mourned', 'moustache', 'moved', 'much', 'muddling', 'multiplied', 'murmur', 'muscles', 'must', 'my', 'myself', 'mysterious', 'naive', 'near', 'nearly', 'negatived', 'nervous', 'nervousness', 'neutral', 'never', 'next', 'no', 'none', 'not', 'note', 'nothing', 'now', 'nymphs', 'oak', 'obituary', 'object', 'objects', 'occurred', 'oddly', 'of', 'off', 'often', 'oh', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'open', 'or', 'other', 'our', 'ourselves', 'out', 'outline', 'oval', 'over', 'own', 'packed', 'paid', 'paint', 'painted', 'painter', 'painting', 'pale', 'paled', 'palm-trees', 'panel', 'panelling', 'pardonable', 'pardoned', 'part', 'passages', 'passing', 'past', 'pastels', 'pathos', 'patient', 'people', 'perceptible', 'perfect', 'persistence', 'persuasively', 'phrase', 'picture', 'pictures', 'pines', 'pink', 'place', 'placed', 'plain', 'platitudes', 'pleased', 'pockets', 'point', 'poised', 'poor', 'portrait', 'posing', 'possessed', 'poverty', 'predicted', 'preliminary', 'presenting', 'prestidigitation', 'pretty', 'previous', 'price', 'pride', 'princely', 'prism', 'problem', 'proclaiming', 'prodigious', 'profusion', 'protest', 'prove', 'public', 'purblind', 'purely', 'pushed', 'put', 'qualities', 'quality', 'queerly', 'question', 'quickly', 'quietly', 'quite', 'quote', 'rain', 'raised', 'random', 'rather', 're', 'real', 'really', 'reared', 'reason', 'reassurance', 'recovering', 'recreated', 'reflected', 'reflection', 'regrets', 'relatively', 'remained', 'remember', 'reminded', 'repeating', 'represented', 'reproduction', 'resented', 'resolve', 'resources', 'rest', 'rich', 'ridiculous', 'robbed', 'romantic', 'room', 'rose', 'rs', 'rule', 'run', 's', 'said', 'same', 'satisfaction', 'savour', 'saw', 'say', 'saying', 'says', 'scorn', 'scornful', 'secret', 'see', 'seemed', 'seen', 'self-confident', 'send', 'sensation', 'sensitive', 'sent', 'serious', 'set', 'sex', 'shade', 'shaking', 'shall', 'she', 'shirked', 'short', 'should', 'shoulder', 'shoulders', 'show', 'showed', 'showy', 'shrug', 'shrugged', 'sight', 'sign', 'silent', 'silver', 'similar', 'simpleton', 'simplifications', 'simply', 'since', 'single', 'sitter', 'sitters', 'sketch', 'skill', 'slight', 'slightly', 'slowly', 'small', 'smile', 'smiling', 'sneer', 'so', 'solace', 'some', 'somebody', 'something', 'spacious', 'spaniel', 'speaking-tubes', 'speculations', 'spite', 'splash', 'square', 'stairs', 'stammer', 'stand', 'standing', 'started', 'stay', 'still', 'stocked', 'stood', 'stopped', 'stopping', 'straddling', 'straight', 'strain', 'straining', 'strange', 'straw', 'stream', 'stroke', 'strokes', 'strolled', 'strongest', 'strongly', 'struck', 'studio', 'stuff', 'subject', 'substantial', 'suburban', 'such', 'suddenly', 'suffered', 'sugar', 'suggested', 'sunburn', 'sunburnt', 'sunlit', 'superb', 'sure', 'surest', 'surface', 'surprise', 'surprised', 'surrounded', 'suspected', 'sweetly', 'sweetness', 'swelling', 'swept', 'swum', 't', 'table', 'take', 'taken', 'talking', 'tea', 'tears', 'technicalities', 'technique', 'tell', 'tells', 'tempting', 'terra-cotta', 'terrace', 'terraces', 'terribly', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'they', 'thin', 'thing', 'things', 'think', 'this', 'thither', 'those', 'though', 'thought', 'three', 'threshold', 'threw', 'through', 'throwing', 'tie', 'till', 'time', 'timorously', 'tinge', 'tips', 'tired', 'to', 'told', 'tone', 'tones', 'too', 'took', 'tottering', 'touched', 'toward', 'trace', 'trade', 'transmute', 'traps', 'travelled', 'tribute', 'tributes', 'tricks', 'tried', 'trouser-presses', 'true', 'truth', 'turned', 'twenty', 'twenty-four', 'twice', 'twirling', 'unaccountable', 'uncertain', 'under', 'underlay', 'underneath', 'understand', 'unexpected', 'untouched', 'unusual', 'up', 'up-stream', 'upon', 'upset', 'upstairs', 'us', 'used', 'usual', 'value', 'varnishing', 'vases', 've', 'veins', 'velveteen', 'very', 'villa', 'vindicated', 'virtuosity', 'vista', 'vocation', 'voice', 'wall', 'wander', 'want', 'wanted', 'wants', 'was', 'wasn', 'watched', 'watching', 'water-colour', 'waves', 'way', 'weekly', 'weeks', 'welcome', 'went', 'were', 'what', 'when', 'whenever', 'where', 'which', 'while', 'white', 'white-panelled', 'who', 'whole', 'whom', 'why', 'wide', 'widow', 'wife', 'wild', 'wincing', 'window-curtains', 'wish', 'with', 'without', 'wits', 'woman', 'women', 'won', 'wonder', 'wondered', 'word', 'work', 'working', 'worth', 'would', 'wouldn', 'year', 'years', 'yellow', 'yet', 'you', 'younger', 'your', 'yourself'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f67854b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3095e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9aeb983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('window-curtains', 1106)\n",
      "('afterward', 139)\n",
      "('azaleas', 190)\n",
      "('beauty', 203)\n",
      "('distinguished', 352)\n",
      "('able', 117)\n",
      "('.', 7)\n",
      "('made', 654)\n",
      "('consummate', 287)\n",
      "('shaking', 874)\n",
      "('packed', 743)\n",
      "('balance', 193)\n",
      "('suspected', 964)\n",
      "('fullest', 479)\n",
      "('own', 742)\n",
      "('toward', 1024)\n",
      "('years', 1123)\n",
      "('glory', 496)\n",
      "('dress-closets', 368)\n",
      "('borne', 224)\n",
      "('demand', 324)\n",
      "('anywhere', 163)\n",
      "('again', 140)\n",
      "('abruptly', 120)\n",
      "('heart', 537)\n",
      "('whenever', 1091)\n",
      "('twirling', 1041)\n",
      "('curiosity', 306)\n",
      "('poor', 781)\n",
      "('bronzes', 233)\n",
      "('Miss', 62)\n",
      "('has', 529)\n",
      "('three', 1004)\n",
      "('attack', 183)\n",
      "('seemed', 863)\n",
      "('wish', 1107)\n",
      "('here', 540)\n",
      "('started', 924)\n",
      "('complex', 282)\n",
      "('managed', 658)\n",
      "('trace', 1025)\n",
      "('corner', 290)\n",
      "('kind', 593)\n",
      "('faith', 425)\n",
      "('wants', 1076)\n",
      "('previous', 791)\n",
      "('s', 850)\n",
      "('buying', 240)\n",
      "('forgive', 462)\n",
      "('grayish', 507)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for item in random.sample(list(vocab.items()), 50):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a136c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # Map IDs to Tokens and Join with Spaces\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9abbc6",
   "metadata": {},
   "source": [
    "\n",
    "- **Clean Up Spaces Before Punctuation**\n",
    "```python\n",
    "text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "```\n",
    "\n",
    "- **Input**: \n",
    "  ```python\n",
    "  text = \"\\\" It 's the last he painted , you know , \\\" Mrs. Gisburn said with pardonable pride .\"\n",
    "  ```\n",
    "\n",
    "- **Regular Expression Explanation**:\n",
    "  - The pattern `\\s+([,.?!\"()\\'])` matches:\n",
    "    - `\\s+`: One or more whitespace characters (spaces, tabs, etc.).\n",
    "    - `([,.?!\"()\\'])`: A capturing group matching any single character from the set: `,`, `.`, `?`, `!`, `\"`, `(`, `)`, `'`.\n",
    "  - The replacement `\\1` keeps the matched punctuation (the captured group) and removes the preceding whitespace.\n",
    "  - This ensures punctuation like commas and periods are not preceded by spaces, making the text look natural.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add58d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\n",
    "\"It's the last he painted, you know,\" \n",
    "Mrs. Gisburn said with pardonable pride.\n",
    "\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27259b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53eb00ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text=\"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32ebc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "To deal with token which not present in vocabulary add \n",
    "\n",
    "Special context tokens to the vocab or to the gvien token\n",
    "\n",
    "1. <code> <|unk|> </code>\n",
    "2. <code> <|endoftext|> </code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d6309ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token : integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f95f5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [ item.strip() for item in preprocessed if item.strip() ]\n",
    "        preprocessed = [ item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed ]\n",
    "        ids = [ self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32aa758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(self, text):\n",
    "#     preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "#     preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "    \n",
    "#     processed_tokens = []\n",
    "#     for item in preprocessed:\n",
    "#         if item in self.str_to_int:\n",
    "#             processed_tokens.append(item)\n",
    "#         else:\n",
    "#             processed_tokens.append(\"<|unk|>\")\n",
    "    \n",
    "#     ids = [self.str_to_int[s] for s in processed_tokens]\n",
    "#     return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <endoftext> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <endoftext> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2698776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ab69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|unk|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753db7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc0e39b6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Byte Pair Encoding\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.9 environment at: D:\\pytorch\\env1\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e75cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d38cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaca969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "}\n",
    "\n",
    "vocab_size = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "for model, size in vocab_size.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefbad7",
   "metadata": {},
   "source": [
    "**Creating Input-Output Target Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:    [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:    {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92403561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05486b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Make sure there is no file named 'torch.py' in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset1(Dataset):\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use the sliding windwos to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca15943",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Step 1 – Initialization**\n",
    "\n",
    "You call something like:\n",
    "\n",
    "```python\n",
    "dataset = GPTDataset1(txt, tokenizer, max_length=6, stride=3)\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* `txt` → Your raw text string (the entire file content).\n",
    "* `tokenizer` → Tokenizes the text into token IDs.\n",
    "* `max_length` → Length of each training sequence.\n",
    "* `stride` → Step size to move the sliding window.\n",
    "\n",
    "\n",
    "\n",
    "**Step 2 – Tokenization**\n",
    "\n",
    "```python\n",
    "token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "```\n",
    "\n",
    "* Converts the **entire text** into a list of integers.\n",
    "  Example:\n",
    "  If `txt = \"Hello world\"` and tokenizer maps:\n",
    "\n",
    "```\n",
    "\"Hello\" → 15496  \n",
    "\"world\" → 995  \n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "token_ids = [15496, 995]\n",
    "```\n",
    "\n",
    "\n",
    "**Step 3 – Sliding Window Loop**\n",
    "\n",
    "```python\n",
    "for i in range(0, len(token_ids)-max_length, stride):\n",
    "```\n",
    "\n",
    "* Moves through `token_ids` with a **window size** of `max_length` and **step size** of `stride`.\n",
    "* `len(token_ids) - max_length` ensures we don’t go out of range.\n",
    "\n",
    "Example:\n",
    "If:\n",
    "\n",
    "```python\n",
    "token_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "max_length = 6\n",
    "stride = 3\n",
    "```\n",
    "\n",
    "The loop runs with:\n",
    "\n",
    "```\n",
    "i = 0 → window covers [1, 2, 3, 4, 5, 6]\n",
    "i = 3 → window covers [4, 5, 6, 7, 8, 9]\n",
    "```\n",
    "\n",
    "\n",
    "**Step 4 – Creating Input & Target Chunks**\n",
    "\n",
    "Inside the loop:\n",
    "\n",
    "```python\n",
    "input_chunk = token_ids[i : i+max_length]\n",
    "target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "```\n",
    "\n",
    "* `input_chunk` → The tokens fed **to the model**.\n",
    "* `target_chunk` → The **shifted-by-one** tokens that the model should **predict**.\n",
    "\n",
    "Example for `i = 0`:\n",
    "\n",
    "```\n",
    "input_chunk  = [1, 2, 3, 4, 5, 6]\n",
    "target_chunk = [2, 3, 4, 5, 6, 7]\n",
    "```\n",
    "\n",
    "\n",
    "**Step 5 – Store as Tensors**\n",
    "\n",
    "```python\n",
    "self.input_ids.append(torch.tensor(input_chunk))\n",
    "self.target_ids.append(torch.tensor(target_chunk))\n",
    "```\n",
    "\n",
    "* Stores each chunk as a **PyTorch tensor** for training.\n",
    "\n",
    "\n",
    "**Step 6 – Dataset Behavior**\n",
    "\n",
    "* `__len__()` → Returns **number of chunks** created.\n",
    "* `__getitem__(index)` → Returns `(input_ids[index], target_ids[index])`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "len(dataset)  →  number of sliding windows\n",
    "dataset[0]    →  (tensor([1, 2, 3, 4, 5, 6]),\n",
    "                  tensor([2, 3, 4, 5, 6, 7]))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDataset1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f65272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, taget = next(data_iter)\n",
    "\n",
    "print(f\"Inputs:\\n {inputs}\")\n",
    "print(f\"Targets:\\n {taget}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b36434",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d905459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e559e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Inputs Shapes:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=8, \n",
    "    max_length=max_length,\n",
    "    stride=max_length,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, tagets = next(data_iter)\n",
    "\n",
    "print(f\"Token IDs:\\n {inputs}\")\n",
    "print(f\"Inputs Shapes:\\n {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138e117",
   "metadata": {},
   "source": [
    "#### **Absolute Positional Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dca953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[-0.6486,  0.6104, -1.3630,  ...,  0.5562,  0.3620,  0.7280],\n",
      "        [-1.1202,  0.9267,  0.0733,  ...,  0.2828,  1.3301,  0.6929],\n",
      "        [ 0.6616,  0.0863, -0.3309,  ...,  0.0087,  0.9789,  0.4809],\n",
      "        [-1.3929,  1.0655, -0.3036,  ..., -0.2142,  0.0666,  0.2834]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b7fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
