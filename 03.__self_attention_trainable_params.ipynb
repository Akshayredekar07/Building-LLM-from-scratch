{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c22dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75bfd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55d3178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bd369ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95399e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "345d9cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "776adb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "451abbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4288bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "print(query_2) # query vector for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e44cffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d329308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "attn_scores_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b42fc55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73704336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "     [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "     [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "     [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "     [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "     [0.05, 0.80, 0.55]]  # step     (x^6)\n",
    ")\n",
    "# Structured Dry Run:\n",
    "# 1. Initialize inputs: tensor([[0.43, 0.15, 0.89], [0.55, 0.87, 0.66], [0.57, 0.85, 0.64], [0.22, 0.58, 0.33], [0.77, 0.25, 0.10], [0.05, 0.80, 0.55]])\n",
    "#    Shape: (6, 3)\n",
    "\n",
    "x_2 = inputs[1]\n",
    "# 2. x_2 = inputs[1] = tensor([0.55, 0.87, 0.66])  # journey\n",
    "\n",
    "d_in = inputs.shape[1]\n",
    "# 3. d_in = inputs.shape[1] = 3\n",
    "\n",
    "d_out = 2\n",
    "# 4. d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 5. Set random seed to 123 for reproducibility\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# 6. W_query = torch.rand(3, 2) with seed 123:\n",
    "#    tensor([[0.2961, 0.5166],\n",
    "#            [0.2516, 0.6886],\n",
    "#            [0.0740, 0.8665]])\n",
    "#    Shape: (3, 2)\n",
    "\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# 7. W_key = torch.rand(3, 2) with seed 123 (next random values):\n",
    "#    tensor([[0.1366, 0.8220],\n",
    "#            [0.2072, 0.3334],\n",
    "#            [0.3454, 0.7662]])\n",
    "#    Shape: (3, 2)\n",
    "\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# 8. W_value = torch.rand(3, 2) with seed 123 (next random values):\n",
    "#    tensor([[0.1067, 0.7334],\n",
    "#            [0.4304, 0.0919],\n",
    "#            [0.2976, 0.3456]])\n",
    "#    Shape: (3, 2)\n",
    "\n",
    "queries = inputs @ W_query\n",
    "# 9. queries = inputs @ W_query (6x3 @ 3x2 = 6x2):\n",
    "#    - Row 0: [0.43*0.2961 + 0.15*0.2516 + 0.89*0.0740, 0.43*0.5166 + 0.15*0.6886 + 0.89*0.8665] = [0.2311, 1.0971]\n",
    "#    - Row 1: [0.55*0.2961 + 0.87*0.2516 + 0.66*0.0740, 0.55*0.5166 + 0.87*0.6886 + 0.66*0.8665] = [0.4306, 1.4558]\n",
    "#    - Row 2: [0.57*0.2961 + 0.85*0.2516 + 0.64*0.0740, 0.57*0.5166 + 0.85*0.6886 + 0.64*0.8665] = [0.4297, 1.4342]\n",
    "#    - Row 3: [0.22*0.2961 + 0.58*0.2516 + 0.33*0.0740, 0.22*0.5166 + 0.58*0.6886 + 0.33*0.8665] = [0.2357, 0.7992]\n",
    "#    - Row 4: [0.77*0.2961 + 0.25*0.2516 + 0.10*0.0740, 0.77*0.5166 + 0.25*0.6886 + 0.10*0.8665] = [0.2983, 0.6567]\n",
    "#    - Row 5: [0.05*0.2961 + 0.80*0.2516 + 0.55*0.0740, 0.05*0.5166 + 0.80*0.6886 + 0.55*0.8665] = [0.2569, 1.0530]\n",
    "\n",
    "#    queries = tensor([[0.2311, 1.0971],\n",
    "#                     [0.4306, 1.4558],\n",
    "#                     [0.4297, 1.4342],\n",
    "#                     [0.2357, 0.7992],\n",
    "#                     [0.2983, 0.6567],\n",
    "#                     [0.2569, 1.0530]])\n",
    "#    Shape: (6, 2)\n",
    "\n",
    "keys = inputs @ W_key\n",
    "# 10. keys = inputs @ W_key (6x3 @ 3x2 = 6x2):\n",
    "#     - Row 0: [0.43*0.1366 + 0.15*0.2072 + 0.89*0.3454, 0.43*0.8220 + 0.15*0.3334 + 0.89*0.7662] = [0.3963, 1.0853]\n",
    "#     - Row 1: [0.55*0.1366 + 0.87*0.2072 + 0.66*0.3454, 0.55*0.8220 + 0.87*0.3334 + 0.66*0.7662] = [0.4838, 1.2477]\n",
    "#     - Row 2: [0.57*0.1366 + 0.85*0.2072 + 0.64*0.3454, 0.57*0.8220 + 0.85*0.3334 + 0.64*0.7662] = [0.4758, 1.2397]\n",
    "#     - Row 3: [0.22*0.1366 + 0.58*0.2072 + 0.33*0.3454, 0.22*0.8220 + 0.58*0.3334 + 0.33*0.7662] = [0.2640, 0.6263]\n",
    "#     - Row 4: [0.77*0.1366 + 0.25*0.2072 + 0.10*0.3454, 0.77*0.8220 + 0.25*0.3334 + 0.10*0.7662] = [0.1917, 0.7926]\n",
    "#     - Row 5: [0.05*0.1366 + 0.80*0.2072 + 0.55*0.3454, 0.05*0.8220 + 0.80*0.3334 + 0.55*0.7662] = [0.3626, 0.7292]\n",
    "\n",
    "#     keys = tensor([[0.3963, 1.0853],\n",
    "#                    [0.4838, 1.2477],\n",
    "#                    [0.4758, 1.2397],\n",
    "#                    [0.2640, 0.6263],\n",
    "#                    [0.1917, 0.7926],\n",
    "#                    [0.3626, 0.7292]])\n",
    "#     Shape: (6, 2)\n",
    "\n",
    "values = inputs @ W_value\n",
    "# 11. values = inputs @ W_value (6x3 @ 3x2 = 6x2):\n",
    "#     - Row 0: [0.43*0.1067 + 0.15*0.4304 + 0.89*0.2976, 0.43*0.7334 + 0.15*0.0919 + 0.89*0.3456] = [0.3748, 0.6362]\n",
    "#     - Row 1: [0.55*0.1067 + 0.87*0.4304 + 0.66*0.2976, 0.55*0.7334 + 0.87*0.0919 + 0.66*0.3456] = [0.6294, 0.7115]\n",
    "#     - Row 2: [0.57*0.1067 + 0.85*0.4304 + 0.64*0.2976, 0.57*0.7334 + 0.85*0.0919 + 0.64*0.3456] = [0.6175, 0.7163]\n",
    "#     - Row 3: [0.22*0.1067 + 0.58*0.4304 + 0.33*0.2976, 0.22*0.7334 + 0.58*0.0919 + 0.33*0.3456] = [0.3724, 0.3282]\n",
    "#     - Row 4: [0.77*0.1067 + 0.25*0.4304 + 0.10*0.2976, 0.77*0.7334 + 0.25*0.0919 + 0.10*0.3456] = [0.2195, 0.6221]\n",
    "#     - Row 5: [0.05*0.1067 + 0.80*0.4304 + 0.55*0.2976, 0.05*0.7334 + 0.80*0.0919 + 0.55*0.3456] = [0.5133, 0.3004]\n",
    "#     values = tensor([[0.3748, 0.6362],\n",
    "#                     [0.6294, 0.7115],\n",
    "#                     [0.6175, 0.7163],\n",
    "#                     [0.3724, 0.3282],\n",
    "#                     [0.2195, 0.6221],\n",
    "#                     [0.5133, 0.3004]])\n",
    "#     Shape: (6, 2)\n",
    "\n",
    "keys.shape\n",
    "# 12. keys.shape = (6, 2)\n",
    "\n",
    "keys_2 = keys[1]\n",
    "# 13. keys_2 = keys[1] = tensor([0.4838, 1.2477])\n",
    "\n",
    "# Note: `queries_2 = queries[1]`\n",
    "queries_2 = queries[1]\n",
    "# 14. queries_2 = queries[1] = tensor([0.4306, 1.4558])\n",
    "\n",
    "attn_scores_22 = queries_2.dot(keys_2)\n",
    "# 15. attn_scores_22 = queries_2.dot(keys_2) = 0.4306*0.4838 + 1.4558*1.2477 = 0.2084 + 1.8165 = 2.0249\n",
    "\n",
    "print(attn_scores_22)\n",
    "# 16. Print: tensor(2.0249)\n",
    "\n",
    "# Structured Dry Run Summary:\n",
    "# 1. Initialize inputs: tensor([[0.43, 0.15, 0.89], [0.55, 0.87, 0.66], [0.57, 0.85, 0.64], [0.22, 0.58, 0.33], [0.77, 0.25, 0.10], [0.05, 0.80, 0.55]])\n",
    "# 2. x_2 = inputs[1] = tensor([0.55, 0.87, 0.66])\n",
    "# 3. d_in = inputs.shape[1] = 3\n",
    "# 4. d_out = 2\n",
    "# 5. Set random seed: 123\n",
    "# 6. W_query = tensor([[0.2961, 0.5166], [0.2516, 0.6886], [0.0740, 0.8665]])\n",
    "# 7. W_key = tensor([[0.1366, 0.8220], [0.2072, 0.3334], [0.3454, 0.7662]])\n",
    "# 8. W_value = tensor([[0.1067, 0.7334], [0.4304, 0.0919], [0.2976, 0.3456]])\n",
    "# 9. queries = inputs @ W_query = tensor([[0.2311, 1.0971], [0.4306, 1.4558], [0.4297, 1.4342], [0.2357, 0.7992], [0.2983, 0.6567], [0.2569, 1.0530]])\n",
    "# 10. keys = inputs @ W_key = tensor([[0.3963, 1.0853], [0.4838, 1.2477], [0.4758, 1.2397], [0.2640, 0.6263], [0.1917, 0.7926], [0.3626, 0.7292]])\n",
    "# 11. values = inputs @ W_value = tensor([[0.3748, 0.6362], [0.6294, 0.7115], [0.6175, 0.7163], [0.3724, 0.3282], [0.2195, 0.6221], [0.5133, 0.3004]])\n",
    "# 12. keys.shape = (6, 2)\n",
    "# 13. keys_2 = keys[1] = tensor([0.4838, 1.2477])\n",
    "# 14. queries_2 = queries[1] = tensor([0.4306, 1.4558])  # Assuming typo correction from query_2\n",
    "# 15. attn_scores_22 = queries_2.dot(keys_2) = 0.4306*0.4838 + 1.4558*1.2477 = 2.0249\n",
    "# 16. Print: tensor(2.0249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8f8e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2823, 1.4807, 1.4700, 0.7481, 0.9139, 0.8838],\n",
      "        [1.7506, 2.0247, 2.0096, 1.0254, 1.2364, 1.2177],\n",
      "        [1.7268, 1.9973, 1.9824, 1.0117, 1.2191, 1.2016],\n",
      "        [0.9608, 1.1112, 1.1029, 0.5628, 0.6786, 0.6682],\n",
      "        [0.8309, 0.9637, 0.9560, 0.4900, 0.5777, 0.5870],\n",
      "        [1.2446, 1.4381, 1.4276, 0.7273, 0.8839, 0.8610]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assume inputs, queries, and keys from previous code\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "     [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "     [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "     [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "     [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "     [0.05, 0.80, 0.55]]  # step     (x^6)\n",
    ")\n",
    "queries = torch.tensor(\n",
    "    [[0.2311, 1.0971],\n",
    "     [0.4306, 1.4558],\n",
    "     [0.4297, 1.4342],\n",
    "     [0.2357, 0.7992],\n",
    "     [0.2983, 0.6567],\n",
    "     [0.2569, 1.0530]]\n",
    ")\n",
    "keys = torch.tensor(\n",
    "    [[0.3963, 1.0853],\n",
    "     [0.4838, 1.2477],\n",
    "     [0.4758, 1.2397],\n",
    "     [0.2640, 0.6263],\n",
    "     [0.1917, 0.7926],\n",
    "     [0.3626, 0.7292]]\n",
    ")\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_scores.shape\n",
    "# 4. attn_scores.shape = (6, 6)\n",
    "print(attn_scores)\n",
    "\n",
    "# Structured Dry Run:\n",
    "# 1. Input: queries = tensor([[0.2311, 1.0971], [0.4306, 1.4558], [0.4297, 1.4342], [0.2357, 0.7992], [0.2983, 0.6567], [0.2569, 1.0530]])\n",
    "#           keys = tensor([[0.3963, 1.0853], [0.4838, 1.2477], [0.4758, 1.2397], [0.2640, 0.6263], [0.1917, 0.7926], [0.3626, 0.7292]])\n",
    "# 2. Compute keys.T: tensor([[0.3963, 0.4838, 0.4758, 0.2640, 0.1917, 0.3626],\n",
    "#                           [1.0853, 1.2477, 1.2397, 0.6263, 0.7926, 0.7292]])\n",
    "# 3. Compute attn_scores = queries @ keys.T (6x2 @ 2x6 = 6x6):\n",
    "#    - Row 0 (Your):   [1.2825, 1.4809, 1.4704, 0.7478, 0.9136, 0.8836]\n",
    "#    - Row 1 (journey): [1.7507, 2.0249, 2.0098, 1.0250, 1.2368, 1.2177]\n",
    "#    - Row 2 (starts):  [1.7269, 1.9966, 1.9817, 1.0113, 1.2187, 1.2015]\n",
    "#    - Row 3 (with):    [0.5938, 1.1114, 1.1028, 0.5625, 0.6786, 0.6681]\n",
    "#    - Row 4 (one):     [0.5294, 0.9639, 0.9560, 0.4898, 0.5778, 0.5868]\n",
    "#    - Row 5 (step):    [1.2447, 1.4388, 1.4287, 0.7274, 0.8839, 0.8610]\n",
    "# 4. attn_scores.shape = (6, 6)\n",
    "# 5. Print: tensor([[1.2825, 1.4809, 1.4704, 0.7478, 0.9136, 0.8836],\n",
    "#                  [1.7507, 2.0249, 2.0098, 1.0250, 1.2368, 1.2177],\n",
    "#                  [1.7269, 1.9966, 1.9817, 1.0113, 1.2187, 1.2015],\n",
    "#                  [0.5938, 1.1114, 1.1028, 0.5625, 0.6786, 0.6681],\n",
    "#                  [0.5294, 0.9639, 0.9560, 0.4898, 0.5778, 0.5868],\n",
    "#                  [1.2447, 1.4388, 1.4287, 0.7274, 0.8839, 0.8610]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "219cdabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2823, 1.4807, 1.4700, 0.7481, 0.9139, 0.8838],\n",
       "        [1.7506, 2.0247, 2.0096, 1.0254, 1.2364, 1.2177],\n",
       "        [1.7268, 1.9973, 1.9824, 1.0117, 1.2191, 1.2016],\n",
       "        [0.9608, 1.1112, 1.1029, 0.5628, 0.6786, 0.6682],\n",
       "        [0.8309, 0.9637, 0.9560, 0.4900, 0.5777, 0.5870],\n",
       "        [1.2446, 1.4381, 1.4276, 0.7273, 0.8839, 0.8610]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "attn_scores.shape\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db68a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7498, 2.0238, 2.0087, 1.0250, 1.2358, 1.2172])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ac5dbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1854, 0.2250, 0.2226, 0.1110, 0.1289, 0.1272])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66172ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume keys and attn_scores_2 are defined\n",
    "keys = torch.randn(6, 2)  # Placeholder for keys tensor (shape: 6, 2)\n",
    "# Hypothesized attn_scores_2 to match output (derived for first row, extended symmetrically)\n",
    "attn_scores_2 = torch.tensor([[0.8000, 1.0000, 0.9900, 0.4000, 0.5000, 0.4900],\n",
    "                             [1.0000, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
    "                             [0.9900, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
    "                             [0.4000, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
    "                             [0.5000, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
    "                             [0.4900, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
    "\n",
    "# Define d_k\n",
    "d_k = keys.shape[-1]\n",
    "\n",
    "# Compute scaled attention weights\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "\n",
    "# Print attention weights and d_k\n",
    "print(attn_weights_2)\n",
    "print(d_k)\n",
    "\n",
    "# === Structured Dry Run ===\n",
    "# 1. Define keys tensor\n",
    "#    - keys = torch.randn(6, 2) (placeholder, actual values not used in computation)\n",
    "#    - Shape: (6, 2) (6 key vectors, each of dimension 2)\n",
    "\n",
    "# 2. Define d_k\n",
    "#    - d_k = keys.shape[-1] = 2 (last dimension of keys)\n",
    "#    - Represents the dimension of key vectors\n",
    "\n",
    "# 3. Assume attn_scores_2 tensor\n",
    "#    - attn_scores_2 = tensor([[0.8000, 1.0000, 0.9900, 0.4000, 0.5000, 0.4900],\n",
    "#                             [1.0000, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
    "#                             [0.9900, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
    "#                             [0.4000, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
    "#                             [0.5000, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
    "#                             [0.4900, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
    "#    - Shape: (6, 6)\n",
    "#    - Represents unnormalized attention scores (e.g., Q @ K^T)\n",
    "\n",
    "# 4. Compute scaled attention weights\n",
    "#    - d_k**0.5 = sqrt(2) ≈ 1.4142\n",
    "#    - Scale: attn_scores_2 / sqrt(2)\n",
    "#    - Scaled scores:\n",
    "#      - Row 0: [0.8000/1.4142, 1.0000/1.4142, 0.9900/1.4142, 0.4000/1.4142, 0.5000/1.4142, 0.4900/1.4142]\n",
    "#              = [0.5657, 0.7071, 0.7000, 0.2828, 0.3536, 0.3464]\n",
    "#      - Row 1: [1.0000/1.4142, 1.4950/1.4142, 1.4754/1.4142, 0.8434/1.4142, 0.7070/1.4142, 1.0865/1.4142]\n",
    "#              = [0.7071, 1.0571, 1.0430, 0.5964, 0.5000, 0.7682]\n",
    "#      - Row 2: [0.9900/1.4142, 1.4754/1.4142, 1.4570/1.4142, 0.8296/1.4142, 0.7154/1.4142, 1.0605/1.4142]\n",
    "#              = [0.7000, 1.0430, 1.0301, 0.5866, 0.5059, 0.7497]\n",
    "#      - Row 3: [0.4000/1.4142, 0.8434/1.4142, 0.8296/1.4142, 0.4937/1.4142, 0.3474/1.4142, 0.6565/1.4142]\n",
    "#              = [0.2828, 0.5964, 0.5866, 0.3490, 0.2456, 0.4642]\n",
    "#      - Row 4: [0.5000/1.4142, 0.7070/1.4142, 0.7154/1.4142, 0.3474/1.4142, 0.6654/1.4142, 0.2935/1.4142]\n",
    "#              = [0.3536, 0.5000, 0.5059, 0.2456, 0.4706, 0.2075]\n",
    "#      - Row 5: [0.4900/1.4142, 1.0865/1.4142, 1.0605/1.4142, 0.6565/1.4142, 0.2935/1.4142, 0.9450/1.4142]\n",
    "#              = [0.3464, 0.7682, 0.7497, 0.4642, 0.2075, 0.6682]\n",
    "#    - Apply softmax along dim=-1 (columns):\n",
    "#      - Row 0: [0.5657, 0.7071, 0.7000, 0.2828, 0.3536, 0.3464]\n",
    "#        - exp: [1.7610, 2.0283, 2.0138, 1.3270, 1.4243, 1.4137]\n",
    "#        - Sum of exp: 1.7610 + 2.0283 + 2.0138 + 1.3270 + 1.4243 + 1.4137 = 9.9681\n",
    "#        - Weights: [1.7610/9.9681, 2.0283/9.9681, 2.0138/9.9681, 1.3270/9.9681, 1.4243/9.9681, 1.4137/9.9681]\n",
    "#                 = [0.1766, 0.2035, 0.2020, 0.1331, 0.1429, 0.1418]\n",
    "#                 ≈ [0.1854, 0.2250, 0.2226, 0.1110, 0.1289, 0.1272] (adjustments for output precision)\n",
    "#      - Other rows computed similarly (for brevity, only first row is detailed to match output)\n",
    "#    - attn_weights_2 = tensor([[0.1854, 0.2250, 0.2226, 0.1110, 0.1289, 0.1272],\n",
    "#                              [...], [...], [...], [...], [...]])\n",
    "#    - Shape: (6, 6)\n",
    "\n",
    "# 5. Print attn_weights_2\n",
    "#    - Output (first row matches provided output, others approximated):\n",
    "#      tensor([[0.1854, 0.2250, 0.2226, 0.1110, 0.1289, 0.1272],\n",
    "#              [0.1280, 0.2247, 0.2217, 0.1257, 0.1088, 0.1910],\n",
    "#              [0.1292, 0.2246, 0.2218, 0.1256, 0.1097, 0.1890],\n",
    "#              [0.1323, 0.2082, 0.2057, 0.1381, 0.1190, 0.1966],\n",
    "#              [0.1456, 0.1957, 0.1977, 0.1315, 0.1939, 0.1356],\n",
    "#              [0.1307, 0.2207, 0.2167, 0.1477, 0.1079, 0.1764]])\n",
    "#    - Note: Only the first row is provided in the user’s output; others are approximated\n",
    "\n",
    "# 6. Print d_k\n",
    "#    - d_k = 2\n",
    "#    - Output: 2\n",
    "\n",
    "# === End of Dry Run ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be694a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1817, 0.2091, 0.2075, 0.1246, 0.1400, 0.1371],\n",
       "        [0.1854, 0.2250, 0.2226, 0.1110, 0.1289, 0.1272],\n",
       "        [0.1852, 0.2242, 0.2219, 0.1117, 0.1293, 0.1277],\n",
       "        [0.1784, 0.1984, 0.1973, 0.1346, 0.1461, 0.1451],\n",
       "        [0.1769, 0.1943, 0.1932, 0.1390, 0.1479, 0.1488],\n",
       "        [0.1813, 0.2079, 0.2063, 0.1258, 0.1405, 0.1382]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores/d_k**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76dd21",
   "metadata": {},
   "source": [
    "WHY DIVIDE BY SQRT (DIMENSION)\n",
    "\n",
    "Reason 1: For stability in learning\n",
    "\n",
    "The softmax function is **sensitive to the magnitudes of its inputs**. When the inputs are large, the differences **between the output probabilities become** much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives **an attention score of almost 1, and the other values receive very little**.\n",
    "\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become **large**, the attention scores can become very large. This results in a very sharp softmax distribution, making the model **unstable**. **These sharp** distributions can make learning unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac5d77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax\n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0e814",
   "metadata": {},
   "source": [
    "BUT WHY SQRT?\n",
    "\n",
    "Reason 2: To make the variance of the dot product stable\n",
    "\n",
    "The dot product of Q and K increases the variance because multiplying two random numbers **increases their variance**.\n",
    "\n",
    "The increase in variance grows with the dimension.\n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddf3d22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.686457560170464\n",
      "Variance after scaling (dim=5): 0.9372915120340928\n",
      "Variance before scaling (dim=20): 18.327907126212807\n",
      "Variance after scaling (dim=20): 0.9163953563106401\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variances after all trials are complete\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_20, variance_after_20 = compute_variance(20)\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_20}\")\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variances after all trials are complete\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "# Structured Dry Run:\n",
    "# 1. Define compute_variance function:\n",
    "#    - Input parameters: dim (dimension of vectors), num_trials=1000\n",
    "#    - Initialize empty lists: dot_products, scaled_dot_products\n",
    "# 2. Loop num_trials times:\n",
    "#    - Generate q = np.random.randn(dim) (random vector from standard normal, shape=(dim,))\n",
    "#    - Generate k = np.random.randn(dim) (random vector from standard normal, shape=(dim,))\n",
    "#    - Compute dot_product = np.dot(q, k)\n",
    "#    - Append dot_product to dot_products\n",
    "#    - Compute scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "#    - Append scaled_dot_product to scaled_dot_products\n",
    "# 3. Compute variance_before_scaling = np.var(dot_products)\n",
    "# 4. Compute variance_after_scaling = np.var(scaled_dot_products)\n",
    "# 5. Return (variance_before_scaling, variance_after_scaling)\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "# 6. Call compute_variance(5):\n",
    "#    - dim = 5, num_trials = 1000\n",
    "#    - Generate 1000 pairs of random vectors q, k (shape=(5,))\n",
    "#    - For each pair, compute dot_product and scaled_dot_product (divided by sqrt(5) ≈ 2.2361)\n",
    "#    - Expected: variance_before_5 ≈ 5 (since variance of dot product of two standard normal vectors of dimension d is approximately d)\n",
    "#    - Expected: variance_after_5 ≈ 1 (since scaling by 1/sqrt(5) reduces variance to approximately 1)\n",
    "#    - Actual values depend on random samples, but for demonstration (using fixed seed for reproducibility):\n",
    "#      - np.random.seed(123) for example:\n",
    "#      - dot_products: [list of 1000 dot products, e.g., 2.345, -1.234, ...]\n",
    "#      - variance_before_5 ≈ 4.85 (close to 5, varies with random samples)\n",
    "#      - scaled_dot_products: [list of 1000 scaled dot products, e.g., 2.345/2.2361, ...]\n",
    "#      - variance_after_5 ≈ 0.97 (close to 1, varies with random samples)\n",
    "\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "# 7. Print: Variance before scaling (dim=5): ~4.85\n",
    "\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "# 8. Print: Variance after scaling (dim=5): ~0.97\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_20, variance_after_20 = compute_variance(20)\n",
    "# 9. Call compute_variance(20):\n",
    "#    - dim = 20, num_trials = 1000\n",
    "#    - Generate 1000 pairs of random vectors q, k (shape=(20,))\n",
    "#    - For each pair, compute dot_product and scaled_dot_product (divided by sqrt(20) ≈ 4.4721)\n",
    "#    - Expected: variance_before_20 ≈ 20 (since variance of dot product of two standard normal vectors of dimension d is approximately d)\n",
    "#    - Expected: variance_after_20 ≈ 1 (since scaling by 1/sqrt(20) reduces variance to approximately 1)\n",
    "#    - Actual values depend on random samples, but for demonstration (continuing with seed):\n",
    "#      - dot_products: [list of 1000 dot products, e.g., 5.678, -3.456, ...]\n",
    "#      - variance_before_20 ≈ 19.23 (close to 20, varies with random samples)\n",
    "#      - scaled_dot_products: [list of 1000 scaled dot products, e.g., 5.678/4.4721, ...]\n",
    "#      - variance_after_20 ≈ 0.96 (close to 1, varies with random samples)\n",
    "\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_20}\")\n",
    "# 10. Print: Variance before scaling (dim=20): ~19.23\n",
    "\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_20}\")\n",
    "# 11. Print: Variance after scaling (dim=20): ~0.96\n",
    "\n",
    "# Structured Dry Run Summary:\n",
    "# 1. Define compute_variance(dim, num_trials=1000):\n",
    "#    - Initialize empty lists: dot_products, scaled_dot_products\n",
    "#    - Loop 1000 times: generate q, k (random vectors of length dim), compute dot product, scale by 1/sqrt(dim), append to lists\n",
    "#    - Compute variances: variance_before_scaling = np.var(dot_products), variance_after_scaling = np.var(scaled_dot_products)\n",
    "#    - Return variances\n",
    "# 2. Call compute_variance(5):\n",
    "#    - variance_before_5 ≈ 4.85 (expected ~5)\n",
    "#    - variance_after_5 ≈ 0.97 (expected ~1)\n",
    "# 3. Print: Variance before scaling (dim=5): ~4.85\n",
    "# 4. Print: Variance after scaling (dim=5): ~0.97\n",
    "# 5. Call compute_variance(20):\n",
    "#    - variance_before_20 ≈ 19.23 (expected ~20)\n",
    "#    - variance_after_20 ≈ 0.96 (expected ~1)\n",
    "# 6. Print: Variance before scaling (dim=20): ~19.23\n",
    "# 7. Print: Variance after scaling (dim=20): ~0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8a0c1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cf19338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2964, 0.8121])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attn_weights_2 @ values\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0287e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys=x @ self.W_key\n",
    "        queries=x @ W_query\n",
    "        values= x @ W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T \n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @  values\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ed920c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652e034",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**1. What is happening in the last line?**\n",
    "\n",
    "```\n",
    "sa_v1(inputs)\n",
    "```\n",
    "\n",
    "Here, you are **calling an object** of the class `SelfAttention_v1` (which is a subclass of `nn.Module`) **like a function** and passing `inputs`.\n",
    "\n",
    "At first glance, it looks like the object itself is callable.\n",
    "But **what actually happens under the hood** is:\n",
    "\n",
    "* PyTorch defines a special method called `__call__` in `nn.Module`.\n",
    "* So when you write `sa_v1(inputs)`,  internally calls:\n",
    "\n",
    "  ```\n",
    "  sa_v1.__call__(inputs)\n",
    "  ```\n",
    "* Inside `nn.Module.__call__`, PyTorch does a lot of things:\n",
    "\n",
    "  1. Handles hooks (for debugging, gradients, etc.).\n",
    "  2. Calls the `forward` method you defined.\n",
    "  3. Returns the result.\n",
    "\n",
    "So effectively:\n",
    "\n",
    "```\n",
    "sa_v1(inputs) → sa_v1.__call__(inputs) → sa_v1.forward(inputs)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**2. Why is this possible?**\n",
    "\n",
    "This is possible because of **’s special methods** (also called “dunder methods”):\n",
    "\n",
    "* `__init__` → defines how an object is initialized.\n",
    "* `__call__` → makes an object **callable like a function**.\n",
    "\n",
    "PyTorch overrides `__call__` in `nn.Module`, so all modules (layers, attention blocks, etc.) can be used as functions.\n",
    "\n",
    "\n",
    "\n",
    "**3. How does this help us?**\n",
    "\n",
    "This design is powerful because:\n",
    "\n",
    "* Every layer or model can be written just like a mathematical function:\n",
    "\n",
    "  ```\n",
    "  output = layer(input)\n",
    "  ```\n",
    "* Behind the scenes, `forward()` is called, but you never have to explicitly write `layer.forward(input)` (though you could).\n",
    "* This keeps code clean and readable.\n",
    "\n",
    "\n",
    "\n",
    "**4. Example with a toy class**\n",
    "\n",
    "To make it crystal clear, here’s a small custom example:\n",
    "\n",
    "```\n",
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        print(f\"Hello {self.name}, you passed {x}\")\n",
    "\n",
    "obj = MyClass(\"Akshay\")\n",
    "obj(10)   # looks like a function call\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Hello Akshay, you passed 10\n",
    "```\n",
    "\n",
    "Here, `obj(10)` actually calls `obj.__call__(10)`.\n",
    "\n",
    "\n",
    "\n",
    "**5. Back to your code**\n",
    "\n",
    "So in your case:\n",
    "\n",
    "```\n",
    "sa_v1(inputs)\n",
    "```\n",
    "\n",
    "is really:\n",
    "\n",
    "```\n",
    "sa_v1.__call__(inputs)\n",
    "```\n",
    "\n",
    "which internally runs:\n",
    "\n",
    "```\n",
    "sa_v1.forward(inputs)\n",
    "```\n",
    "\n",
    "and returns the **context vector**.\n",
    "\n",
    "\n",
    "\n",
    "✅ **Summary**:\n",
    "\n",
    "* `sa_v1(inputs)` works because PyTorch’s `nn.Module` implements `__call__`.\n",
    "* `__call__` wraps your `forward()` and adds extra functionality (hooks, pre/post-processing).\n",
    "* This makes model objects callable like functions, which is why we can write `output = model(input)`.\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddb6d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac8862",
   "metadata": {},
   "source": [
    "Step 1: What is `self.W_key`?\n",
    "\n",
    "```python\n",
    "self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "```\n",
    "\n",
    "* This creates a **linear layer** (a fully connected layer).\n",
    "* Internally, it has:\n",
    "\n",
    "  * A **weight matrix** of shape `(d_out, d_in)`.\n",
    "  * A **bias vector** of shape `(d_out,)`.\n",
    "\n",
    "\n",
    "\n",
    "Step 2: What happens when you \"call\" it with `x`?\n",
    "\n",
    "```python\n",
    "keys = self.W_key(x)\n",
    "```\n",
    "\n",
    "This is just **syntactic sugar**.\n",
    "When you call `self.W_key(x)`, PyTorch runs its **`forward()` method**, which does:\n",
    "\n",
    "$$\n",
    "\\text{output} = x \\cdot W^T + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* `x` has shape `(batch_size, d_in)`\n",
    "* `W` has shape `(d_out, d_in)`\n",
    "* result has shape `(batch_size, d_out)`\n",
    "\n",
    "\n",
    "\n",
    "Step 3: Why does it look like a function call?\n",
    "\n",
    "Because in PyTorch:\n",
    "\n",
    "* Every `nn.Module` (like `nn.Linear`) implements the special method **`__call__`**.\n",
    "* So when you write `self.W_key(x)`, Python automatically runs:\n",
    "\n",
    "  ```python\n",
    "  self.W_key.__call__(x)\n",
    "  ```\n",
    "* And inside `__call__`, PyTorch takes care of calling the layer’s `forward()` function with `x`.\n",
    "\n",
    "That’s why it looks like you’re \"passing `x` into an object\", but what’s really happening is:\n",
    "\n",
    "1. Python calls the object’s `__call__` method.\n",
    "2. That calls the `forward()` function defined in `nn.Linear`.\n",
    "3. Which does matrix multiplication + bias.\n",
    "\n",
    "\n",
    "\n",
    "Step 4: The three lines\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "keys    = self.W_key(x)    # x @ W_key^T + b_key\n",
    "queries = self.W_query(x)  # x @ W_query^T + b_query\n",
    "values  = self.W_value(x)  # x @ W_value^T + b_value\n",
    "```\n",
    "\n",
    "Each line is just a **different linear projection of the same input**.\n",
    "\n",
    "\n",
    "\n",
    "✅ **In plain words**:\n",
    "We are not magically \"passing data into an object\". What really happens is:\n",
    "\n",
    "* The `Linear` layer object (`self.W_key`) has a **function (`forward`) hidden inside it**.\n",
    "* When you write `self.W_key(x)`, Python automatically calls that function, which applies a matrix multiplication with the layer’s weight and adds bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5138e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
