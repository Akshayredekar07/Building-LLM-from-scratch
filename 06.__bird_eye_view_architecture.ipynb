{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c04d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa17407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc6062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        ) \n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg['\"emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'], cfg['vocab_size'], bias=False\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fa4d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa9f8452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b1c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8b8b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the manual seed for random number generation to 123\n",
    "# This ensures reproducibility; all random operations will produce the same results on every run\n",
    "# No output here, just initializes the random state\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Generate a random tensor 'batch_example' of shape (2, 5)\n",
    "# - 2: Batch size (number of examples)\n",
    "# - 5: Number of features per example\n",
    "# Values are sampled from a standard normal distribution (mean=0, std=1)\n",
    "# Due to the seed, the exact values produced are:\n",
    "# tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
    "#         [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
    "# This tensor represents a small batch of data, e.g., inputs to a model\n",
    "batch_example = torch.randn(2, 5)  # Labelled as #A in the original code, perhaps for reference later\n",
    "\n",
    "# Define a sequential neural network layer\n",
    "# nn.Sequential: Chains modules in order; output of one is input to the next\n",
    "# - nn.Linear(5, 6): Linear (fully connected) layer\n",
    "#   - Input features: 5 (matches the last dimension of batch_example)\n",
    "#   - Output features: 6\n",
    "#   - Applies: output = input @ weight.T + bias\n",
    "#   - Weights and bias are randomly initialized (reproducibly due to seed)\n",
    "# - nn.ReLU(): Rectified Linear Unit activation\n",
    "#   - Applies element-wise: max(0, x) to introduce non-linearity\n",
    "# No computation yet; this just builds the model structure\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "\n",
    "# Pass the batch_example through the layer\n",
    "# Step 1: Linear transformation\n",
    "# - Input shape: (2, 5) -> Output shape: (2, 6)\n",
    "# - Computations happen matrix-wise for the batch\n",
    "# - Exact intermediate values after linear (before ReLU):\n",
    "#   tensor([[ 0.2260,  0.3470, -0.4727,  0.2216, -0.1220, -0.8747],\n",
    "#           [ 0.2133,  0.2394, -0.1502,  0.5198,  0.3297, -0.2985]])\n",
    "# Step 2: ReLU activation\n",
    "# - Replaces all negative values with 0, keeps positives unchanged\n",
    "# - Final output shape remains (2, 6)\n",
    "# - Exact values:\n",
    "#   tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "#           [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "# The 'grad_fn' indicates this tensor is part of a computation graph for backpropagation\n",
    "out = layer(batch_example)\n",
    "\n",
    "# Print the output tensor\n",
    "# This would display the final tensor as shown above in the console\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20967eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2260,  0.3470, -0.4727,  0.2216, -0.1220, -0.8747],\n",
      "        [ 0.2133,  0.2394, -0.1502,  0.5198,  0.3297, -0.2985]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5,6))\n",
    "\n",
    "# out=layer(batch_example)\n",
    "\n",
    "# out = layer.__call__(out)\n",
    "\n",
    "out = layer.forward(batch_example)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842092e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5])\n",
      "Output shape: torch.Size([2, 6])\n",
      "Output: tensor([[ 0.2260,  0.3470, -0.4727,  0.2216, -0.1220, -0.8747],\n",
      "        [ 0.2133,  0.2394, -0.1502,  0.5198,  0.3297, -0.2985]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_example = torch.randn(2, 5)  # 2 samples(batch), 5 features each\n",
    "\n",
    "# Correct: nn.Linear(input_features, output_features)\n",
    "layer = nn.Sequential(nn.Linear(5, 6))  # 5 input features, 6 output features\n",
    "\n",
    "out = layer(batch_example)\n",
    "\n",
    "print(\"Input shape:\", batch_example.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Output:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a33b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5])\n",
      "Weight shape: torch.Size([6, 5])\n",
      "Output shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "# Manual implementation\n",
    "weight = torch.randn(6, 5)  # (output_features, input_features)\n",
    "bias = torch.randn(6)       # (output_features)\n",
    "\n",
    "# Matrix multiplication: (2,5) × (5,6) = (2,6)\n",
    "out = torch.matmul(batch_example, weight.t()) + bias\n",
    "\n",
    "print(\"Input shape:\", batch_example.shape)\n",
    "print(\"Weight shape:\", weight.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d78eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5])\n",
      "Layer weight shape: torch.Size([6, 5])\n",
      "Layer bias shape: torch.Size([6])\n",
      "Output shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "# Create the linear layer\n",
    "linear_layer = nn.Linear(5, 6)  # 5 in, 6 out\n",
    "\n",
    "# Explicit forward pass\n",
    "out = linear_layer.forward(batch_example)\n",
    "\n",
    "print(\"Input shape:\", batch_example.shape)\n",
    "print(\"Layer weight shape:\", linear_layer.weight.shape)  # Should be (6, 5)\n",
    "print(\"Layer bias shape:\", linear_layer.bias.shape)      # Should be (6,)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24438316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2260,  0.3470, -0.4727,  0.2216, -0.1220, -0.8747],\n",
       "        [ 0.2133,  0.2394, -0.1502,  0.5198,  0.3297, -0.2985]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "230a2876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.1125],\n",
      "        [ 0.1423]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.2296],\n",
      "        [0.0944]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Assume 'out' is the tensor from the previous code\n",
    "# out: tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "#             [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "# Shape: (2, 6), where 2 is the batch size and 6 is the number of features per example\n",
    "\n",
    "# Compute the mean along the last dimension (dim=-1, i.e., dim=1)\n",
    "# - For each batch example (row), calculate the mean across the 6 features\n",
    "# - keepdim=True ensures the output keeps the dimension as (2, 1) instead of (2,)\n",
    "# - Calculation:\n",
    "#   - First row: (0.2260 + 0.3470 + 0.0000 + 0.2216 + 0.0000 + 0.0000) / 6 = 0.7946 / 6 ≈ 0.1324\n",
    "#   - Second row: (0.2133 + 0.2394 + 0.0000 + 0.5198 + 0.3297 + 0.0000) / 6 = 1.3022 / 6 ≈ 0.2170\n",
    "# - Resulting mean tensor:\n",
    "#   tensor([[0.1324],\n",
    "#           [0.2170]])\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute the variance along the last dimension (dim=-1, i.e., dim=1)\n",
    "# - Variance (unbiased, uses n-1 denominator by default in PyTorch):\n",
    "#   - For each row, compute: sum((x_i - mean)^2) / (n-1), where n=6\n",
    "#   - First row:\n",
    "#     - Mean = 0.1324\n",
    "#     - Deviations: [0.2260-0.1324, 0.3470-0.1324, 0.0000-0.1324, 0.2216-0.1324, 0.0000-0.1324, 0.0000-0.1324]\n",
    "#                = [0.0936, 0.2146, -0.1324, 0.0892, -0.1324, -0.1324]\n",
    "#     - Squared deviations: [0.008761, 0.046053, 0.017530, 0.007956, 0.017530, 0.017530]\n",
    "#     - Sum of squared deviations: 0.11536\n",
    "#     - Variance: 0.11536 / (6-1) = 0.11536 / 5 ≈ 0.0231\n",
    "#   - Second row:\n",
    "#     - Mean = 0.2170\n",
    "#     - Deviations: [0.2133-0.2170, 0.2394-0.2170, 0.0000-0.2170, 0.5198-0.2170, 0.3297-0.2170, 0.0000-0.2170]\n",
    "#                = [-0.0037, 0.0224, -0.2170, 0.3028, 0.1127, -0.2170]\n",
    "#     - Squared deviations: [0.000014, 0.000502, 0.047089, 0.091688, 0.012701, 0.047089]\n",
    "#     - Sum of squared deviations: 0.199083\n",
    "#     - Variance: 0.199083 / (6-1) = 0.199083 / 5 ≈ 0.0398\n",
    "# - Resulting variance tensor:\n",
    "#   tensor([[0.0231],\n",
    "#           [0.0398]])\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "# Print the mean tensor\n",
    "# Output will be:\n",
    "# Mean:\n",
    "# tensor([[0.1324],\n",
    "#         [0.2170]])\n",
    "print(\"Mean:\\n\", mean)\n",
    "\n",
    "# Print the variance tensor\n",
    "# Output will be:\n",
    "# Variance:\n",
    "# tensor([[0.0231],\n",
    "#         [0.0398]])\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume inputs from previous code:\n",
    "# out: tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "#             [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "# Shape: (2, 6)\n",
    "# mean: tensor([[0.1324],\n",
    "#               [0.2170]])\n",
    "# Shape: (2, 1)\n",
    "# var: tensor([[0.0231],\n",
    "#              [0.0398]])\n",
    "# Shape: (2, 1)\n",
    "\n",
    "# Normalize the output: (out - mean) / sqrt(var)\n",
    "# - This is the core of layer normalization (without epsilon or learnable parameters gamma, beta)\n",
    "# - Broadcasting occurs: mean (2, 1) and var (2, 1) are applied to each feature in out (2, 6)\n",
    "# - Step 1: Subtract mean\n",
    "#   - First row: [0.2260-0.1324, 0.3470-0.1324, 0.0000-0.1324, 0.2216-0.1324, 0.0000-0.1324, 0.0000-0.1324]\n",
    "#              = [0.0936, 0.2146, -0.1324, 0.0892, -0.1324, -0.1324]\n",
    "#   - Second row: [0.2133-0.2170, 0.2394-0.2170, 0.0000-0.2170, 0.5198-0.2170, 0.3297-0.2170, 0.0000-0.2170]\n",
    "#               = [-0.0037, 0.0224, -0.2170, 0.3028, 0.1127, -0.2170]\n",
    "# - Step 2: Divide by sqrt(var)\n",
    "#   - sqrt(var): [sqrt(0.0231) ≈ 0.1520, sqrt(0.0398) ≈ 0.1995]\n",
    "#   - First row: [0.0936/0.1520, 0.2146/0.1520, -0.1324/0.1520, 0.0892/0.1520, -0.1324/0.1520, -0.1324/0.1520]\n",
    "#              ≈ [0.6158, 1.4118, -0.8711, 0.5868, -0.8711, -0.8711]\n",
    "#   - Second row: [-0.0037/0.1995, 0.0224/0.1995, -0.2170/0.1995, 0.3028/0.1995, 0.1127/0.1995, -0.2170/0.1995]\n",
    "#               ≈ [-0.0185, 0.1123, -1.0877, 1.5173, 0.5649, -1.0877]\n",
    "# - Resulting normalized tensor:\n",
    "#   tensor([[ 0.6158,  1.4118, -0.8711,  0.5868, -0.8711, -0.8711],\n",
    "#           [-0.0185,  0.1123, -1.0877,  1.5173,  0.5649, -1.0877]])\n",
    "# - Shape remains (2, 6)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "\n",
    "# Compute the mean of the normalized output along the last dimension (dim=-1)\n",
    "# - For each batch example, calculate the mean across the 6 features\n",
    "# - keepdim=True ensures output shape is (2, 1)\n",
    "# - Calculation:\n",
    "#   - First row: (0.6158 + 1.4118 + (-0.8711) + 0.5868 + (-0.8711) + (-0.8711)) / 6\n",
    "#              ≈ 0.0011 / 6 ≈ 0.0002 (very close to 0 due to normalization)\n",
    "#   - Second row: (-0.0185 + 0.1123 + (-1.0877) + 1.5173 + 0.5649 + (-1.0877)) / 6\n",
    "#               ≈ -0.0014 / 6 ≈ -0.0002 (very close to 0 due to normalization)\n",
    "# - Resulting mean tensor:\n",
    "#   tensor([[ 0.0002],\n",
    "#           [-0.0002]])\n",
    "# - Values are not exactly 0 due to floating-point precision\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute the variance of the normalized output along the last dimension (dim=-1)\n",
    "# - Variance (unbiased, n-1 denominator):\n",
    "#   - For each row, compute: sum((x_i - mean)^2) / (n-1), where n=6\n",
    "#   - First row:\n",
    "#     - Mean ≈ 0.0002\n",
    "#     - Deviations: [0.6158-0.0002, 1.4118-0.0002, -0.8711-0.0002, 0.5868-0.0002, -0.8711-0.0002, -0.8711-0.0002]\n",
    "#                ≈ [0.6156, 1.4116, -0.8713, 0.5866, -0.8713, -0.8713]\n",
    "#     - Squared deviations: [0.3790, 1.9936, 0.7592, 0.3441, 0.7592, 0.7592]\n",
    "#     - Sum of squared deviations: 4.9943\n",
    "#     - Variance: 4.9943 / (6-1) ≈ 0.9989 (close to 1 due to normalization)\n",
    "#   - Second row:\n",
    "#     - Mean ≈ -0.0002\n",
    "#     - Deviations: [-0.0185-(-0.0002), 0.1123-(-0.0002), -1.0877-(-0.0002), 1.5173-(-0.0002), 0.5649-(-0.0002), -1.0877-(-0.0002)]\n",
    "#                ≈ [-0.0183, 0.1125, -1.0875, 1.5175, 0.5651, -1.0875]\n",
    "#     - Squared deviations: [0.0003, 0.0127, 1.1829, 2.3026, 0.3193, 1.1829]\n",
    "#     - Sum of squared deviations: 5.0007\n",
    "#     - Variance: 5.0007 / (6-1) ≈ 1.0001 (close to 1 due to normalization)\n",
    "# - Resulting variance tensor:\n",
    "#   tensor([[0.9989],\n",
    "#           [1.0001]])\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "# Print the normalized output tensor\n",
    "# Output will be:\n",
    "# Normalized layer outputs:\n",
    "# tensor([[ 0.6158,  1.4118, -0.8711,  0.5868, -0.8711, -0.8711],\n",
    "#         [-0.0185,  0.1123, -1.0877,  1.5173,  0.5649, -1.0877]])\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "# Print the mean tensor\n",
    "# Output will be:\n",
    "# Mean:\n",
    "# tensor([[ 0.0002],\n",
    "#         [-0.0002]])\n",
    "print(\"Mean:\\n\", mean)\n",
    "\n",
    "# Print the variance tensor\n",
    "# Output will be:\n",
    "# Variance:\n",
    "# tensor([[0.9989],\n",
    "#         [1.0001]])\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ad4ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18ae03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LayerNorm class, inheriting from nn.Module\n",
    "class LayerNorm(nn.Module):\n",
    "    # Constructor: Initialize the layer normalization module\n",
    "    def __init__(self, emb_dim):\n",
    "        # Call the parent class (nn.Module) constructor\n",
    "        super().__init__()\n",
    "        # Define epsilon for numerical stability in normalization\n",
    "        # eps = 1e-5 prevents division by zero in sqrt(var + eps)\n",
    "        self.eps = 1e-5\n",
    "        # Define learnable scale parameter (gamma in layer norm)\n",
    "        # Initialized as a tensor of ones, shape (emb_dim,)\n",
    "        # nn.Parameter makes it trainable, e.g., for emb_dim=6: tensor([1., 1., 1., 1., 1., 1.])\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        # Define learnable shift parameter (beta in layer norm)\n",
    "        # Initialized as a tensor of zeros, shape (emb_dim,)\n",
    "        # For emb_dim=6: tensor([0., 0., 0., 0., 0., 0.])\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    # Forward pass: Define how input tensor x is processed\n",
    "    def forward(self, x):\n",
    "        # Compute mean across the last dimension (dim=-1, i.e., features)\n",
    "        # For input shape (2, 6), output shape is (2, 1)\n",
    "        # Using previous out tensor:\n",
    "        # - First row: (0.2260 + 0.3470 + 0.0000 + 0.2216 + 0.0000 + 0.0000) / 6 ≈ 0.1324\n",
    "        # - Second row: (0.2133 + 0.2394 + 0.0000 + 0.5198 + 0.3297 + 0.0000) / 6 ≈ 0.2170\n",
    "        # Result: tensor([[0.1324], [0.2170]])\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute variance across the last dimension (dim=-1)\n",
    "        # unbiased=False uses n (not n-1) in denominator for population variance\n",
    "        # Formula: sum((x_i - mean)^2) / n, where n=6\n",
    "        # - First row:\n",
    "        #   - Deviations: [0.2260-0.1324, 0.3470-0.1324, ..., 0.0000-0.1324] = [0.0936, 0.2146, -0.1324, 0.0892, -0.1324, -0.1324]\n",
    "        #   - Squared: [0.008761, 0.046053, 0.017530, 0.007956, 0.017530, 0.017530]\n",
    "        #   - Sum: 0.11536\n",
    "        #   - Variance: 0.11536 / 6 ≈ 0.0192\n",
    "        # - Second row:\n",
    "        #   - Deviations: [-0.0037, 0.0224, -0.2170, 0.3028, 0.1127, -0.2170]\n",
    "        #   - Squared: [0.000014, 0.000502, 0.047089, 0.091688, 0.012701, 0.047089]\n",
    "        #   - Sum: 0.199083\n",
    "        #   - Variance: 0.199083 / 6 ≈ 0.0332\n",
    "        # Result: tensor([[0.0192], [0.0332]])\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize: (x - mean) / sqrt(var + eps)\n",
    "        # - Add eps=1e-5 to variance for numerical stability\n",
    "        # - sqrt(var + eps): [sqrt(0.0192 + 1e-5) ≈ 0.1386, sqrt(0.0332 + 1e-5) ≈ 0.1822]\n",
    "        # - Subtract mean (broadcasting (2, 1) to (2, 6)):\n",
    "        #   - First row: [0.0936, 0.2146, -0.1324, 0.0892, -0.1324, -0.1324]\n",
    "        #   - Second row: [-0.0037, 0.0224, -0.2170, 0.3028, 0.1127, -0.2170]\n",
    "        # - Divide by sqrt(var + eps):\n",
    "        #   - First row: [0.0936/0.1386, 0.2146/0.1386, -0.1324/0.1386, 0.0892/0.1386, -0.1324/0.1386, -0.1324/0.1386]\n",
    "        #              ≈ [0.6753, 1.5483, -0.9554, 0.6437, -0.9554, -0.9554]\n",
    "        #   - Second row: [-0.0037/0.1822, 0.0224/0.1822, -0.2170/0.1822, 0.3028/0.1822, 0.1127/0.1822, -0.2170/0.1822]\n",
    "        #               ≈ [-0.0203, 0.1229, -1.1909, 1.6615, 0.6184, -1.1909]\n",
    "        # - Result: tensor([[ 0.6753,  1.5483, -0.9554,  0.6437, -0.9554, -0.9554],\n",
    "        #                  [-0.0203,  0.1229, -1.1909,  1.6615,  0.6184, -1.1909]])\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Apply learnable scale and shift\n",
    "        # - scale: tensor([1., 1., 1., 1., 1., 1.]) (shape (6,))\n",
    "        # - shift: tensor([0., 0., 0., 0., 0., 0.]) (shape (6,))\n",
    "        # - Broadcasting: scale and shift are applied to each feature across the batch\n",
    "        # - Since scale=1 and shift=0, the output is unchanged: norm_x * 1 + 0 = norm_x\n",
    "        # - Output shape: (2, 6)\n",
    "        # - Final output:\n",
    "        #   tensor([[ 0.6753,  1.5483, -0.9554,  0.6437, -0.9554, -0.9554],\n",
    "        #           [-0.0203,  0.1229, -1.1909,  1.6615,  0.6184, -1.1909]])\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "# Example usage (not in the provided code, but to complete the dry run):\n",
    "# Instantiate LayerNorm with emb_dim=6 (since out has 6 features)\n",
    "layer_norm = LayerNorm(emb_dim=6)\n",
    "\n",
    "# Pass the previous out tensor through the layer\n",
    "# out: tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "#              [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]])\n",
    "out_normalized = layer_norm(out)\n",
    "\n",
    "# The output would be as computed above:\n",
    "# tensor([[ 0.6753,  1.5483, -0.9554,  0.6437, -0.9554, -0.9554],\n",
    "#         [-0.0203,  0.1229, -1.1909,  1.6615,  0.6184, -1.1909]])\n",
    "\n",
    "# To verify, compute mean and variance of out_normalized (similar to previous code):\n",
    "mean = out_normalized.mean(dim=-1, keepdim=True)\n",
    "# - First row: (0.6753 + 1.5483 + (-0.9554) + 0.6437 + (-0.9554) + (-0.9554)) / 6 ≈ 0.0002\n",
    "# - Second row: (-0.0203 + 0.1229 + (-1.1909) + 1.6615 + 0.6184 + (-1.1909)) / 6 ≈ 0.0001\n",
    "# Result: tensor([[0.0002], [0.0001]]) (very close to 0)\n",
    "\n",
    "var = out_normalized.var(dim=-1, keepdim=True, unbiased=False)\n",
    "# - First row: sum([0.6753-0.0002, ...]^2) / 6 ≈ 1.0000\n",
    "# - Second row: sum([-0.0203-0.0001, ...]^2) / 6 ≈ 1.0000\n",
    "# Result: tensor([[1.0000], [1.0000]]) (very close to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9e6bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67e099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
